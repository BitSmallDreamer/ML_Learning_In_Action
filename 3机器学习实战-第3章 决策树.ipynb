{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#第3章-决策树\" data-toc-modified-id=\"第3章-决策树-1\">第3章 决策树</a></span><ul class=\"toc-item\"><li><span><a href=\"#本章内容\" data-toc-modified-id=\"本章内容-1.1\">本章内容</a></span></li><li><span><a href=\"#决策树的构造\" data-toc-modified-id=\"决策树的构造-1.2\">决策树的构造</a></span></li><li><span><a href=\"#决策树的一般流程\" data-toc-modified-id=\"决策树的一般流程-1.3\">决策树的一般流程</a></span></li><li><span><a href=\"#信息论\" data-toc-modified-id=\"信息论-1.4\">信息论</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.信息熵\" data-toc-modified-id=\"1.信息熵-1.4.1\">1.信息熵</a></span></li><li><span><a href=\"#2.条件熵\" data-toc-modified-id=\"2.条件熵-1.4.2\">2.条件熵</a></span></li><li><span><a href=\"#3.信息增益\" data-toc-modified-id=\"3.信息增益-1.4.3\">3.信息增益</a></span></li></ul></li><li><span><a href=\"#代码实现与解读\" data-toc-modified-id=\"代码实现与解读-1.5\">代码实现与解读</a></span></li><li><span><a href=\"#摘要\" data-toc-modified-id=\"摘要-1.6\">摘要</a></span></li><li><span><a href=\"#CART决策树\" data-toc-modified-id=\"CART决策树-1.7\">CART决策树</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.CART分类树\" data-toc-modified-id=\"1.CART分类树-1.7.1\">1.CART分类树</a></span></li></ul></li><li><span><a href=\"#决策树-原理\" data-toc-modified-id=\"决策树-原理-1.8\">决策树 原理</a></span><ul class=\"toc-item\"><li><span><a href=\"#决策树-须知概念\" data-toc-modified-id=\"决策树-须知概念-1.8.1\">决策树 须知概念</a></span><ul class=\"toc-item\"><li><span><a href=\"#信息熵-&amp;-信息增益\" data-toc-modified-id=\"信息熵-&amp;-信息增益-1.8.1.1\">信息熵 &amp; 信息增益</a></span></li></ul></li><li><span><a href=\"#决策树-工作原理\" data-toc-modified-id=\"决策树-工作原理-1.8.2\">决策树 工作原理</a></span><ul class=\"toc-item\"><li><span><a href=\"#决策树-开发流程\" data-toc-modified-id=\"决策树-开发流程-1.8.2.1\">决策树 开发流程</a></span></li></ul></li></ul></li><li><span><a href=\"#决策树算法\" data-toc-modified-id=\"决策树算法-1.9\">决策树算法</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章 决策树\n",
    "\n",
    "## 本章内容\n",
    ">* 决策树简介  \n",
    "* 在数据集中度量一致性  \n",
    "* 使用递归构造决策树     \n",
    "* 使用Matplotlib绘制树形图   \n",
    "\n",
    "\n",
    "## 决策树的构造\n",
    ">优点：计算复杂度不高，输出易于理解，对中间值得确实不敏感，可以处理不相关特征数据。  \n",
    "缺点：可能会产生过度匹配问题  \n",
    "使用数据类型：数值型和标称型  \n",
    "\n",
    "**创建分支伪代码函数createBranch()如下：**\n",
    "```\n",
    "检测数据集中的每个指向是否属于同一个分类：\n",
    "    IF so return 类标签\n",
    "    Else\n",
    "        寻找划分数据集的最好特征\n",
    "        划分数据集\n",
    "        创建分支节点\n",
    "            for 每个划分的子集\n",
    "                调用函数createBranch并增加返回结果到分支节点中\n",
    "        return 分支节点\n",
    "```\n",
    "\n",
    "上述是一个递归函数\n",
    "\n",
    "## 决策树的一般流程\n",
    ">(1) 收集数据：可以使用任何方法。  \n",
    "(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。  \n",
    "(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。  \n",
    "(4) 训练算法：构造树的数据结构。  \n",
    "(5) 测试算法：使用经验树计算错误率。   \n",
    "(6) 使用算法：词步骤可以使用于任何监督学习算法，而使用决策树可能更好地理解数据的内在含义。  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "** 摘要**\n",
    "\n",
    "1. 信息论相关知识\n",
    "2. 决策树算法原理\n",
    "3. 代码实现与解释   \n",
    "\n",
    "\n",
    "今天总结决策树算法，目前建立决策树有三种主要算法：ID3、C4.5以及CART。由于算法知识点比较琐碎，我分成两节来总结。\n",
    "\n",
    "第一节主要是梳理决策树算法中ID3和C4.5的知识点；第二节主要梳理剪枝技术、CART算法和随机森林算法的知识。\n",
    "\n",
    "## 信息论 \n",
    "\n",
    "### 1.信息熵  \n",
    "\n",
    "在决策树算法中，熵是一个非常非常重要的概念。\n",
    "\n",
    "一件事发生的概率越小，我们说它所蕴含的信息量越大。\n",
    "\n",
    "比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。\n",
    "\n",
    "所以我们这样衡量信息量：\n",
    "\n",
    "$$\n",
    "i(y)=-log{P(y)}\n",
    "$$\n",
    "其中，$P(y)$是事件发生的概率。\n",
    "\n",
    "信息熵就是所有可能发生事件的信息量的期望：  \n",
    "\n",
    "$$\n",
    "H(Y)=-\\sum_{i=1}^{n}P(y_i)log{P(y_i)}\n",
    "$$\n",
    "表达了$Y$事件发生的不确定度。  \n",
    "\n",
    "### 2.条件熵\n",
    "\n",
    "条件熵：表示在X给定条件下，$Y$的条件概率分布的熵对$X$的数学期望。其数学推导如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} % requires amsmath; align* for no eq. number\n",
    "H(Y|X) & =\\sum_{x\\in{X}}P{(x)}H(Y|X=x) \\\\\n",
    "   & =-\\sum_{x\\in{X}}P(x)\\sum_{y\\in{Y}}P(y|x)log{P(y|x)}\\\\\n",
    "   & =-\\sum_{x\\in{X}}\\sum_{y\\in{Y}}P(x,y)log{P(y|x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "条件熵$H（Y|X）$表示在已知随机变量$X$的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量$X$的条件下（变量$X$的每个值都会取到），另一个变量$Y$的熵对$X$的期望。\n",
    "\n",
    "举个例子\n",
    "\n",
    "例：女生决定主不主动追一个男生的标准有两个：颜值和身高，如下表所示：\n",
    "\n",
    "||颜值|身高|追不追|\n",
    "|--|--|--|--|\n",
    "|1|帅|高|追|\n",
    "|2|帅|不高|追|\n",
    "|3|不帅|高|不追|\n",
    "\n",
    "上表中随机变量$Y=\\{追，不追\\}$，$P(Y=追)=2/3$，$P(Y=不追)=1/3$，得到$Y$的熵：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} % requires amsmath; align* for no eq. number\n",
    "H(Y) & =-\\frac{2}{3}log\\frac{2}{3}-\\frac{1}{3}log\\frac{1}{3} \\\\\n",
    "   & =0.918\n",
    "\\end{aligned}\n",
    "$$\n",
    "这里还有一个特征变量$X$，$X=｛高，不高｝$。当$X=高$时，追的个数为1，占1/2，不追的个数为1，占1/2，此时：\n",
    "\n",
    "$$\n",
    "H(Y|X=高)=-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "同理：\n",
    "\n",
    "$$\n",
    "H(Y|X=不高)=-{1}log{1}-{1}log{1}\n",
    "$$\n",
    "\n",
    "（注意：我们一般约定，当$p=0$时，$plogp=0$）\n",
    "\n",
    "\n",
    "所以我们得到条件熵的计算公式：  \n",
    "\n",
    "$$\n",
    "\\begin{aligned} % requires amsmath; align* for no eq. number\n",
    "H(Y|X=身高) & =P(X=不高)*H(Y|X=不高)+P(X=高)*H(Y|X=高)\\\\\n",
    "            & =0.67\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 3.信息增益 \n",
    "\n",
    "当我们用另一个变量$X$对原变量$Y$分类后，原变量$Y$的不确定性就会减小了（即熵值减小）。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：\n",
    "$$\n",
    "Gain(Y,X)=H(Y)-H(Y|X)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "此外，信息论中还有互信息、交叉熵等概念，它们与本算法关系不大，这里不展开。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 代码实现与解读\n",
    "\n",
    "\n",
    "**1.计算给定数据的香浓熵 **    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算给定数据集的香农熵\n",
    "#从math中导入log函数\n",
    "from math import log\n",
    "import operator\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)   #计算实例中的个数\n",
    "    \n",
    "    labelCounts = {}    #创建字典，键为标签，值为个数\n",
    "   \n",
    "    for featVec in dataSet:    #the the number of unique elements and their occurance\n",
    "        \n",
    "        currentLabel = featVec[-1]    #得到标签，注意是最后一个标签\n",
    "       \n",
    "        if currentLabel not in labelCounts.keys():     #如果标签不在字典已经存在的键中\n",
    "            \n",
    "            labelCounts[currentLabel] = 0       #创建名为currentLabel的键，赋值为0\n",
    "          \n",
    "        labelCounts[currentLabel] += 1     #标签为currentLabel的个数加1       \n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries    #计算每一个标签的概率p\n",
    "        \n",
    "        shannonEnt -= prob * log(prob,2)    #log base 2利用公式计算香农熵\n",
    "        \n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']],\n",
       " ['no surfacing', 'flippers'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "              [1, 1, 'yes'],\n",
    "              [1, 0, 'no'],\n",
    "              [0, 1, 'no'],\n",
    "              [0, 1, 'no']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    #change to discrete values\n",
    "    return dataSet, labels\n",
    "myDat,labels=createDataSet()\n",
    "myDat,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcShannonEnt(myDat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.创建选取的数据特征属性划分数据集**\n",
    "\n",
    "\n",
    "程序清单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照给定特征划分数据集\n",
    "#参数解释：dataSet待划分数据集\n",
    "#axis：划分数据集的特征，这个函数里指函数第几列\n",
    "#value：特征返回值，指的是特征划分的标准\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []     #创建一个新列表\n",
    "  \n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:   #如果这组数据特征值等于特征返回值的话\n",
    "            \n",
    "            reducedFeatVec = featVec[:axis]       #这两行是把原来的数据除掉划分数据的特征那一列 \n",
    "            \n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)   #把列表reduceFeatVect放入retDataSet中\n",
    "            \n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myDat,0,1) \n",
    "# myDat=[1, 1, 'yes'],\n",
    "#       [1, 1, 'yes'],\n",
    "#       [1, 0, 'no'],\n",
    "#       [0, 1, 'no'],\n",
    "#       [0, 1, 'no']\n",
    "# 将myDat的第1列按照取出所有等于1的方式划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'no'], [1, 'no']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myDat,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.根据信息增益准则，选取最好的划分特征**\n",
    "\n",
    "程序清单：  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找到最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1    #得到特征个数，减1是因为类别栏     #the last column is used for the labels\n",
    "   \n",
    "   \n",
    "    baseEntropy = calcShannonEnt(dataSet)   #计算数据原始香农熵\n",
    "   \n",
    "    bestInfoGain = 0.0; bestFeature = -1   #初始化信息增益和初始化最优特征\n",
    "    for i in range(numFeatures):       \n",
    "        \n",
    "        featList = [example[i] for example in dataSet]  #熟悉这种写法，括号里面是取了第i个特征的所有值\n",
    "        \n",
    "        uniqueVals = set(featList)    #set()，生成一个集合数据类型，其和列表类型一样，不同之处在于\n",
    "                                      #集合数据类型里面的值不重复，是唯一的\n",
    "        \n",
    "        newEntropy = 0.0    #初始化新熵为0\n",
    "        \n",
    "        for value in uniqueVals:    #下面这个for函数主要为了计算按第i个特征划分的新熵\n",
    "           \n",
    "            subDataSet = splitDataSet(dataSet, i, value)    #生成按value值划分的数据集\n",
    "            \n",
    "            prob = len(subDataSet)/float(len(dataSet))   #计算概率\n",
    "            \n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)    #计算新熵 \n",
    "       \n",
    "        infoGain = baseEntropy - newEntropy     #计算信息增益calculate the info gain; ie reduction in entropy\n",
    "        \n",
    "        if (infoGain > bestInfoGain):      #得到最大的信息增益和选取特征 #compare this to the best gain so far\n",
    "            bestInfoGain = infoGain         #if better than current best, set to best\n",
    "            bestFeature = i\n",
    "    return bestFeature                      #returns an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numFeatures=2\n",
      "原始熵是： 0.9709505944546686\n",
      "第0个特征的所有取值 [1, 1, 1, 0, 0]\n",
      "-简化取值: {0, 1}\n",
      "--按照0划分取值 [[1, 'no'], [1, 'no']]\n",
      "---去此值的概率是： 0.4\n",
      "---新熵是 0.0\n",
      "--按照1划分取值 [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "---去此值的概率是： 0.6\n",
      "---新熵是 0.5509775004326937\n",
      "-----信息增益 0.4199730940219749\n",
      "此时最好的熵是 0.4199730940219749 此时最佳特征值是 0\n",
      "第1个特征的所有取值 [1, 1, 0, 1, 1]\n",
      "-简化取值: {0, 1}\n",
      "--按照0划分取值 [[1, 'no']]\n",
      "---去此值的概率是： 0.2\n",
      "---新熵是 0.0\n",
      "--按照1划分取值 [[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n",
      "---去此值的概率是： 0.8\n",
      "---新熵是 0.8\n",
      "-----信息增益 0.17095059445466854\n",
      "此时最好的熵是 0.4199730940219749 此时最佳特征值是 0\n"
     ]
    }
   ],
   "source": [
    "# myDat=[1, 1, 'yes'],\n",
    "#       [1, 1, 'yes'],\n",
    "#       [1, 0, 'no'],\n",
    "#       [0, 1, 'no'],\n",
    "#       [0, 1, 'no']\n",
    "numFeatures = len(myDat[0]) - 1 #得到特征个数，减1是因为类别栏     #the last column is used for the labels\n",
    "    #计算数据原始香农熵\n",
    "# numFeatures\n",
    "baseEntropy = calcShannonEnt(myDat)\n",
    "print(\"numFeatures=%d\" %numFeatures) \n",
    "print(\"原始熵是：\",baseEntropy)\n",
    "\n",
    "\n",
    "    #初始化信息增益和初始化最优特征\n",
    "bestInfoGain = 0.0; bestFeature = -1     \n",
    "\n",
    "\n",
    "for i in range(numFeatures):        #iterate over all the features\n",
    "        #熟悉这种写法，括号里面是取了第i个特征的所有值\n",
    "    featList = [example[i] for example in myDat]\n",
    "    print(\"第%d个特征的所有取值\" %i,featList)\n",
    "    \n",
    "    uniqueVals = set(featList) \n",
    "    #初始化新熵为0#get a set of unique values\n",
    "    newEntropy = 0.0\n",
    "        #下面这个for函数主要为了计算按第i个特征划分的新熵\n",
    "    print(\"-简化取值:\",uniqueVals)\n",
    "    for value in uniqueVals:\n",
    "            #生成按value值划分的数据集\n",
    "        subDataSet = splitDataSet(myDat, i, value)\n",
    "        print(\"--按照%d划分取值\"% value,subDataSet)\n",
    "            #计算概率\n",
    "        prob = len(subDataSet)/float(len(myDat))\n",
    "            #计算新熵\n",
    "        print(\"---去此值的概率是：\",prob)\n",
    "        newEntropy += prob * calcShannonEnt(subDataSet)   \n",
    "            #计算信息增益\n",
    "        print(\"---新熵是\",newEntropy)\n",
    "    infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy\n",
    "    print(\"-----信息增益\",infoGain)\n",
    "        #得到最大的信息增益和选取特征\n",
    "    \n",
    "    if (infoGain > bestInfoGain):       #compare this to the best gain so far\n",
    "        bestInfoGain = infoGain         #if better than current best, set to best\n",
    "        bestFeature = i\n",
    "    \n",
    "    print(\"此时最好的熵是\",bestInfoGain,\"此时最佳特征值是\",bestFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chooseBestFeatureToSplit(myDat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从数据集构造决策树算法：其工作原理如下：**\n",
    "1. 得到原始数据集  \n",
    "2. 基于最好的属性值划分数据集（可能存在大于两个分支的数据集划分）    \n",
    "3. 第一次划分后，数据被向下传递到树分支的下一个节点（可以用递归的思想）\n",
    "\n",
    "\n",
    "**递归的条件： **    \n",
    "程序遍历完所有划分数据集属性，或者每个分支下的所有实例都具有相同的分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.多数表决器**\n",
    "\n",
    "\n",
    "程序清单：  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多数表决器\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    \n",
    "    classCount={}\n",
    "    #for程序用来计数\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): \n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    #排序函数\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.创建决策树**  \n",
    "\n",
    "程序清单：  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建树\n",
    "#     myDat  = [[1, 1, 'yes'],\n",
    "#               [1, 1, 'yes'],\n",
    "#               [1, 0, 'no'],\n",
    "#               [0, 1, 'no'],\n",
    "#               [0, 1, 'no']]\n",
    "#     labels = ['no surfacing','flippers']\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList = [example[-1] for example in dataSet]#classLsit里面是dataSet里面的标签\n",
    "    # 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行\n",
    "    # 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。\n",
    "    # count() 函数是统计括号中的值在list中出现的次数\n",
    "    if classList.count(classList[0]) == len(classList): #第一个终止条件：所有类标签都相同，country（）函数用来计数0\n",
    "        return classList[0]#stop splitting when all of the classes are equal\n",
    "    # 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果\n",
    "    # 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。\n",
    "    if len(dataSet[0]) == 1: #第二个终止条件：用完了所有的特征#stop splitting when there are no more features in dataSet\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]#得到标签里的所有属性值\n",
    "    uniqueVals = set(featValues)#得到属性值集合\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "    return myTree                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createTree(myDat,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.使用决策树进行分类**\n",
    "\n",
    "程序清单：  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用决策树分类函数\n",
    "#三个参数意义：input：决策树；featLabels：特征标签；testVec：测试向量\n",
    "\n",
    "def classify(inputTree,featLabels,testVec):\n",
    "    firstStr = inputTree.keys()[0]\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    key = testVec[featIndex]\n",
    "    valueOfFeat = secondDict[key]\n",
    "    if isinstance(valueOfFeat, dict): \n",
    "        classLabel = classify(valueOfFeat, featLabels, testVec)\n",
    "    else: classLabel = valueOfFeat\n",
    "    return classLabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.决策树在磁盘中的存储与导入**  \n",
    "\n",
    "程序清单：  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将决策树分类器存储在磁盘中，filename一般保存为txt格式\n",
    "\n",
    "def storeTree(inputTree,filename):\n",
    "    import pickle\n",
    "    fw = open(filename,'w')\n",
    "    pickle.dump(inputTree,fw)\n",
    "    fw.close()\n",
    "    \n",
    "#将磁盘中的对象加载出来，这里filename就是上面函数中的txt文件\n",
    "    \n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename)\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-33c9af9c39fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreateTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# storeTree(myTree,'classifierStorage.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# grabTree('classifierStorage.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-854ee28d5c1d>\u001b[0m in \u001b[0;36mcreateTree\u001b[1;34m(dataSet, labels)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muniqueVals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0msubLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m       \u001b[1;31m#copy all of labels, so trees don't mess up existing labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmyTree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestFeatLabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestFeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmyTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-854ee28d5c1d>\u001b[0m in \u001b[0;36mcreateTree\u001b[1;34m(dataSet, labels)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmajorityCnt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mbestFeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchooseBestFeatureToSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mbestFeatLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestFeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mmyTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbestFeatLabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestFeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "createTree(myDat,labels)\n",
    "# storeTree(myTree,'classifierStorage.txt')\n",
    "# grabTree('classifierStorage.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要\n",
    "\n",
    "1. CART决策树\n",
    "2. 决策树的剪枝技术\n",
    "3. Bagging与随机森林\n",
    "4. 决策树中缺失值的处理\n",
    "5. 决策树代码\n",
    "\n",
    "注：本节代码对应第九章“树回归”\n",
    "\n",
    "## CART决策树\n",
    "\n",
    "为什么同样作为建立决策树的三种算法之一，我们要将CART算法单独拿出来讲。\n",
    "\n",
    "因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。\n",
    "\n",
    "对于分类树，CART采用基尼指数最小化准则；对于回归树，CART采用平方误差最小化准则\n",
    "\n",
    "\n",
    "### 1.CART分类树\n",
    "\n",
    "CART分类树与上一节讲述的ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。\n",
    "\n",
    "Gini指数作为一种做特征选择的方式，其表征了特征的不纯度。\n",
    "\n",
    "在具体的分类问题中，对于数据集D，我们假设有K个类别，每个类别出现的概率为$P_k$，则数据集$D$的基尼指数的表达式为：\n",
    "\n",
    "$$\n",
    "Gini=1-\\sum_{k=1}^{K}{P_k}^2\n",
    "$$\n",
    "\n",
    "我们取一个极端情况，如果数据集合中的类别只有一类，那么：\n",
    "\n",
    "$$\n",
    "Gini(D)=0\n",
    "$$\n",
    "\n",
    "我们发现，当只有一类时，数据的不纯度是最低的，所以Gini指数等于零。Gini(D)越小，则数据集D的纯度越高。\n",
    "\n",
    "特别地，对于样本D，如果我们选择特征A的某个值a，把D分成$D_1$和$D_2$两部分，则此时，Gini指数为：  \n",
    "\n",
    "$$\n",
    "Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)\n",
    "$$\n",
    "\n",
    "与信息增益类似，我们可以计算如下表达式：  \n",
    "\n",
    "$$\n",
    "\\Delta{Gini(A)}=Gini(D)-Gini(D,A)\n",
    "$$\n",
    "即以特征A划分后，数据不纯度减少的程度。显然，我们在做特征选取时，应该选择最大的一个。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']],\n",
       " ['no surfacing', 'flippers'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for featVec in myDat: #the the number of unique elements and their occurance\n",
    "    currentLabel = featVec[-1]\n",
    "currentLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 2, 'no': 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCounts = {}#创建字典，键为标签，值为个数\n",
    "for featVec in myDat: #the the number of unique elements and their occurance\n",
    "    currentLabel = featVec[-1]#得到标签，注意是最后一个标签\n",
    "    if currentLabel not in labelCounts.keys(): #如果标签不在字典已经存在的键中\n",
    "        labelCounts[currentLabel] = 0#创建名为currentLabel的键，赋值为0\n",
    "    labelCounts[currentLabel] += 1#标签为currentLabel的个数加1\n",
    "labelCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0]\n",
      "[1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "numFeatures = len(myDat[0]) - 1\n",
    "for i in range(numFeatures):\n",
    "    featList = [example[i] for example in myDat]\n",
    "    print(featList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 'yes']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numFeatures = len(myDat[0]) - 1\n",
    "numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'yes', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classList = [example[-1] for example in myDat]\n",
    "classList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestFeat = chooseBestFeatureToSplit(myDat)\n",
    "bestFeatLabel = labels[bestFeat]\n",
    "myTree = {bestFeatLabel:{}}\n",
    "myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(labels[bestFeat])\n",
    "featValues = [example[bestFeat] for example in myDat]\n",
    "featValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们完成了决策树算法原理和主要代码的学习。\n",
    "\n",
    "下一节我们将学习CART算法、随机森林算法以及剪枝技术。\n",
    "\n",
    "以上原理部分主要来自于《机器学习》—周志华，《统计学习方法》—李航，《机器学习实战》—Peter Harrington。代码部分主要来自于《机器学习实战》，我对代码进行了版本的改进，文中代码用Python3实现，这是机器学习主流语言，本人也会尽力对代码做出较为详尽的注释。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 决策树 原理\n",
    "### 决策树 须知概念\n",
    "#### 信息熵 & 信息增益\n",
    "**熵**： 熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。\n",
    "\n",
    "**信息熵（香农熵）**： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。\n",
    "\n",
    "信息增益： 在划分数据集前后信息发生的变化称为信息增益。\n",
    "\n",
    "### 决策树 工作原理\n",
    "如何构造一个决策树?\n",
    "我们使用 createBranch() 方法，如下所示：\n",
    "\n",
    ">检测数据集中的所有数据的分类标签是否相同:   \n",
    "         If so return 类标签    \n",
    "            Else:    \n",
    "                寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）    \n",
    "                划分数据集    \n",
    "                创建分支节点    \n",
    "                        for 每个划分的子集    \n",
    "                                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中   \n",
    "                return 分支节点    \n",
    "        \n",
    "        \n",
    "        \n",
    "#### 决策树 开发流程  \n",
    "\n",
    ">收集数据：可以使用任何方法。   \n",
    "准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。\n",
    "分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。   \n",
    "训练算法：构造树的数据结构。   \n",
    "测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）   \n",
    "使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。     \n",
    "\n",
    "\n",
    "**决策树 算法特点**\n",
    ">优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。   \n",
    "缺点：可能会产生过度匹配问题。   \n",
    "适用数据类型：数值型和标称型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     \n",
    "## 决策树算法  \n",
    "\n",
    "**1.算法简介**\n",
    "\n",
    "决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。\n",
    "\n",
    "以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。\n",
    "\n",
    "**举个例子**\n",
    "\n",
    "有一个划分是不是鸟类的数据集合，如下：\n",
    "\n",
    "||是否会飞|是否会跑|属于鸟类|\n",
    "|--|--|--|--|\n",
    "|1|是|是|否|\n",
    "|2|是|是|是|\n",
    "|3|是|否|否|\n",
    "|4|否|是|否|\n",
    "|5|否|是|否|\n",
    "\n",
    "\n",
    "这时候我们建立这样一个决策树：  \n",
    "\n",
    "<img src=\"https://pic4.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg\",width=400,height=400>\n",
    "\n",
    "\n",
    "当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：  \n",
    "\n",
    "<img src=\"https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg\",width=400,eight=400>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回： \n",
    "\n",
    "(1)当前节点包含的样本属于同一种类，无需划分；\n",
    "\n",
    "(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；\n",
    "\n",
    "(3)当前节点包含的样本集合为空，无法划分。\n",
    "\n",
    "\n",
    "\n",
    "**2.属性选择**\n",
    "\n",
    "在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。\n",
    "\n",
    "\n",
    "\n",
    "**2.1 ID3算法**\n",
    "\n",
    "ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。\n",
    "\n",
    "\n",
    "\n",
    "**2.2 C4.5算法**\n",
    "\n",
    "实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：  \n",
    "\n",
    "$$\n",
    "Gain\\_ratio(Y,X)=\\frac{Gain(Y,X)}{H(X)}\n",
    "$$  \n",
    "\n",
    "\n",
    "其中，分子为信息增益，分母为属性$X$的熵。\n",
    "\n",
    "需要注意的是，增益率准则对可取值数目较少的属性有所偏好。\n",
    "\n",
    "所以一般这样选取划分属性：**先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。\n",
    "\n",
    "\n",
    "**2.3 CART算法**\n",
    "\n",
    "ID3算法和C4.5算法主要存在三个问题：\n",
    "\n",
    "(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。\n",
    "\n",
    "(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。\n",
    "\n",
    "(3)会产生过拟合问题。\n",
    "\n",
    "为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。\n",
    "\n",
    "上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
