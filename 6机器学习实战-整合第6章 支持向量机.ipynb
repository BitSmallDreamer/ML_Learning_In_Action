{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#《机器学习实战》——支持向量机SVM\" data-toc-modified-id=\"《机器学习实战》——支持向量机SVM-1\">《机器学习实战》——支持向量机SVM</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于最大间隔分隔数据\" data-toc-modified-id=\"基于最大间隔分隔数据-1.1\">基于最大间隔分隔数据</a></span></li><li><span><a href=\"#支持向量机-场景\" data-toc-modified-id=\"支持向量机-场景-1.2\">支持向量机 场景</a></span></li><li><span><a href=\"#支持向量机-原理\" data-toc-modified-id=\"支持向量机-原理-1.3\">支持向量机 原理</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-工作原理\" data-toc-modified-id=\"SVM-工作原理-1.3.1\">SVM 工作原理</a></span></li></ul></li><li><span><a href=\"#寻找最大间隔\" data-toc-modified-id=\"寻找最大间隔-1.4\">寻找最大间隔</a></span><ul class=\"toc-item\"><li><span><a href=\"#为什么寻找最大间隔\" data-toc-modified-id=\"为什么寻找最大间隔-1.4.1\">为什么寻找最大间隔</a></span></li><li><span><a href=\"#怎么寻找最大间隔\" data-toc-modified-id=\"怎么寻找最大间隔-1.4.2\">怎么寻找最大间隔</a></span></li></ul></li><li><span><a href=\"#函数间隔与几何间隔\" data-toc-modified-id=\"函数间隔与几何间隔-1.5\">函数间隔与几何间隔</a></span></li><li><span><a href=\"#函数间隔\" data-toc-modified-id=\"函数间隔-1.6\">函数间隔</a></span><ul class=\"toc-item\"><li><span><a href=\"#几何间隔\" data-toc-modified-id=\"几何间隔-1.6.1\">几何间隔</a></span></li><li><span><a href=\"#间隔最大化\" data-toc-modified-id=\"间隔最大化-1.6.2\">间隔最大化</a></span></li><li><span><a href=\"#拉格朗日乘子法\" data-toc-modified-id=\"拉格朗日乘子法-1.6.3\">拉格朗日乘子法</a></span></li><li><span><a href=\"#对偶问题\" data-toc-modified-id=\"对偶问题-1.6.4\">对偶问题</a></span></li><li><span><a href=\"#第1部分\" data-toc-modified-id=\"第1部分-1.6.5\">第1部分</a></span></li><li><span><a href=\"#支持向量机\" data-toc-modified-id=\"支持向量机-1.6.6\">支持向量机</a></span></li><li><span><a href=\"#对偶问题\" data-toc-modified-id=\"对偶问题-1.6.7\">对偶问题</a></span></li></ul></li><li><span><a href=\"#SMO-高效优化算法\" data-toc-modified-id=\"SMO-高效优化算法-1.7\">SMO 高效优化算法</a></span></li><li><span><a href=\"#SVM-开发流程\" data-toc-modified-id=\"SVM-开发流程-1.8\">SVM 开发流程</a></span></li><li><span><a href=\"#代码实现与注释\" data-toc-modified-id=\"代码实现与注释-1.9\">代码实现与注释</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.SMO算法中辅助函数\" data-toc-modified-id=\"1.SMO算法中辅助函数-1.9.1\">1.SMO算法中辅助函数</a></span></li><li><span><a href=\"#2.简化版SMO算法\" data-toc-modified-id=\"2.简化版SMO算法-1.9.2\">2.简化版SMO算法</a></span></li><li><span><a href=\"#3.完整版SMO支持函数\" data-toc-modified-id=\"3.完整版SMO支持函数-1.9.3\">3.完整版SMO支持函数</a></span></li><li><span><a href=\"#4.完整的SMO优化例程\" data-toc-modified-id=\"4.完整的SMO优化例程-1.9.4\">4.完整的SMO优化例程</a></span></li><li><span><a href=\"#第2部分\" data-toc-modified-id=\"第2部分-1.9.5\">第2部分</a></span></li><li><span><a href=\"#线性支持向量机\" data-toc-modified-id=\"线性支持向量机-1.9.6\">线性支持向量机</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《机器学习实战》——支持向量机SVM\n",
    "\n",
    "**本章内容**\n",
    "> 简单介绍支持向量机  \n",
    "利用SMO进行优化   \n",
    "利用核函数对数据进行空间转换  \n",
    "将SVM和其他分类器进行对比  \n",
    "\n",
    "*SVM有很多实现，这里只关注其中最流行的实现，即**序列最小优化（Sequential Minimal Optimization,SMO）**。算法。在这之后，将介绍使用一种称为**核函数（kernal）**的方式将SVM扩展到更多的数据集上。*\n",
    "\n",
    "\n",
    "## 基于最大间隔分隔数据 \n",
    "**支持向量机**\n",
    ">优点：泛化错误率低，计算开销不大，结果易于解释  \n",
    "缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。  \n",
    "适用数据类型：数值型和标称型数据\n",
    "\n",
    "## 支持向量机 场景\n",
    "\n",
    "*线性可分* \n",
    "\n",
    "*分隔超平面 *    \n",
    "\n",
    "*超平面  * \n",
    "\n",
    "*间隔  *\n",
    "\n",
    "\n",
    "![](https://pic2.zhimg.com/80/v2-76a9113248d467dbeeae97a067eac273_hd.jpg)\n",
    "**支持向量（supportvector）**：就是离分隔超平面最近的那些点\n",
    "\n",
    "\n",
    "## 支持向量机 原理\n",
    "\n",
    "### SVM 工作原理\n",
    "\n",
    "![](https://pic3.zhimg.com/80/v2-00becdd15361c8e5ceb65da02bcf7fda_hd.jpg)\n",
    "\n",
    "对于上述的苹果和香蕉，我们想象为2种水果类型的炸弹。（保证距离最近的炸弹，距离它们最远）\n",
    "\n",
    "1. 寻找最大分类间距\n",
    "2. 转而通过拉格朗日函数求优化的问题\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* 数据可以通过画一条直线就可以将它们完全分开，这组数据叫线性可分(linearly separable)数据，而这条分隔直线称为分隔超平面(separating hyperplane)。\n",
    "\n",
    "* 如果数据集上升到1024维呢？那么需要1023维来分隔数据集，也就说需要N-1维的对象来分隔，这个对象叫做超平面(hyperlane)，也就是分类的决策边界。\n",
    "\n",
    "\n",
    "![](https://pic3.zhimg.com/80/v2-52e336604b1667069143100d152513d0_hd.jpg)\n",
    "\n",
    "## 寻找最大间隔\n",
    "\n",
    "### 为什么寻找最大间隔\n",
    "```\n",
    "摘录地址：http://slideplayer.com/slide/8610144  (第12条信息)\n",
    "Support Vector Machines: Slide 12 Copyright © 2001, 2003, Andrew W. Moore Why Maximum Margin? \n",
    "denotes +1 denotes -1 f(x,w,b) = sign(w. x - b) The maximum margin linear classifier is the linear classifier with the, um, maximum margin. \n",
    "This is the simplest kind of SVM (Called an LSVM) Support Vectors are those datapoints that the margin pushes up against \n",
    "\n",
    "1.Intuitively this feels safest. \n",
    "2.If we’ve made a small error in the location of the boundary (it’s been jolted in its perpendicular direction) this gives us least chance of causing a misclassification. \n",
    "3.CV is easy since the model is immune to removal of any non-support-vector datapoints. \n",
    "4.There’s some theory that this is a good thing. \n",
    "5.Empirically it works very very well. \n",
    "\n",
    "*******\n",
    "\n",
    "1. 直觉上是安全的\n",
    "2. 如果我们在边界的位置发生了一个小错误（它在垂直方向上被颠倒），这给我们最小的错误分类机会。\n",
    "3. CV（Computer Vision 计算机视觉 - 这缩写看着可怕�）很容易，因为该模型对任何非支持向量数据点的去除是免疫的。\n",
    "4. 有一些理论，这是一件好事。\n",
    "5. 通常它的工作非常好。\n",
    "```\n",
    "### 怎么寻找最大间隔\n",
    "\n",
    "> 点到超平面的距离\n",
    "\n",
    "* 分隔超平面函数间距: $y(x)=w^Tx+b$\n",
    "\n",
    "* 分类的结果： $f(x)=sign(w^Tx+b)$ ($sign$表示>0为1，<0为-1，=0为0)\n",
    "\n",
    "* 点到超平面的几何间距: $d(x)=(w^Tx+b)/||w||$ （$||w||$表示$w$矩阵的二范式=> $\\sqrt{w*w^T}$, 点到超平面的距离也是类似的）\n",
    "\n",
    "点到直线的距离公式\n",
    "$$\n",
    "d=\\Bigg|\\frac{Ax_0+By_0+C}{\\sqrt{A^2+B^2}}\\Bigg|\n",
    "$$\n",
    "\n",
    "## 函数间隔与几何间隔\n",
    "\n",
    "下面给大家介绍两个概念，函数间隔和几何间隔。\n",
    "\n",
    "## 函数间隔\n",
    "\n",
    "函数间隔：对于给定的训练样本集$D$和超平面$(w,b)$，定义样本点$(x_i,y_i)$到超平面的函数间隔为：\n",
    "$$\\hat\\gamma_i=y_i(w\\cdot x_i+b)$$\n",
    "定义超平面$(w,b)$到训练集$D$的函数间隔为所有样本点的函数间隔的最小值，即：\n",
    "\n",
    "$$\\hat\\gamma=\\min_{i=1...N}\\hat\\gamma_i$$\n",
    "\n",
    "函数间隔总是正数，因为当样本为负类时，$y_i$和$(w\\cdot{x_i}+b)$都小于零。\n",
    "\n",
    "函数间隔表示了分类预测的**相对正确性和确信度**，因为$(w\\cdot{x_i}+b)$越大，说明样本点离分离超平面越远，分类正确的可能性也就越高。\n",
    "\n",
    "同时我们需要格外注意一下，对于同一个样本点和分离超平面，函数间隔可能会不一样。这句话应该怎么理解呢？我们如果成比例地改变系数$w$和$b$，比如把$w$和$b$变成$2w$和$2b$，分离超平面没有发生任何改变，但是对于样本点$(x_i,y_i)$，函数间隔却变成了原来的两倍。所以函数间隔只能表示分类预测的**相对正确性和确信度**。\n",
    "\n",
    "### 几何间隔\n",
    "\n",
    "几何间隔我们可以简单理解为**实际的距离**。\n",
    "\n",
    "样本空间中点$(x_i,y_i)$到超平面的几何间隔为：\n",
    "$$\\gamma_i=y_i\\frac{w\\cdot x_i+b}{||{w}||}$$\n",
    "\n",
    "定义超平面$(w,b)$到训练集D的几何间隔为所有样本点的几何间隔的最小值，即：\n",
    "\n",
    "$$\\gamma=\\min_{i=1...N}\\gamma_i$$\n",
    "\n",
    "函数间隔和几何间隔有如下关系：\n",
    "\n",
    "$$\\gamma=\\frac{\\hat\\gamma}{||{w}||}$$\n",
    "\n",
    "当$||w||=1$时，两者相等。如果超平面参数$w$和$b$成比例的改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。\n",
    "\n",
    "### 间隔最大化\n",
    "\n",
    "SVM的基本思想就是求解能够正确划分数据集并且几何间隔最大的分离超平面。通俗讲就是样本点到分离超平面的最小间隔的最大化，即对最难分的样本点（离超平面最近的点）也有足够大的确信度将它们分开。\n",
    "\n",
    "可以表示成如下最优化问题：\n",
    "\n",
    "$$\n",
    "\\begin{array}.\n",
    "\\max_{w b}\\gamma  \\\\\n",
    "s.t\\ y_i\\frac{w\\cdot{x}+b}{||w||}\\geq\\gamma,i=1,2...N\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "即我们希望最大化数据集的几何间隔。这时我们将几何间隔和函数间隔进行转换，得：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}.\n",
    "\\max_{w b}\\frac{\\hat\\gamma}{||w||}  \\\\\n",
    "s.t\\ y_i\\frac{w\\cdot{x}+b}{||w||}\\geq\\gamma,i=1,2...N\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "函数间隔的取值并不影响最优化问题的解。这句话是下面化简步骤的核心，怎么理解这句话呢？\n",
    "\n",
    "+ *上述优化问题中有三个变量${\\hat\\gamma}$、$w$和$b$  \n",
    "比如说 ${\\hat\\gamma=1}$时，$w=w_1$，$b=b_1$  \n",
    "而 ${\\hat\\gamma=2}$时，$w=2w_1$，$b=2b_1$  \n",
    "所以 ${\\hat\\gamma}$ 的取值对不等式约束和函数优化没有任何影响，  \n",
    "只是同乘一个比例系数  \n",
    "这也是我们之前强调的函数间隔相对性的原因\n",
    "所以不妨令$\\hat\\gamma=1$，并且最大化 $\\frac{1}{\\|{w}\\|}$ 等价于最小化 $\\frac{1}{2}{\\|w\\|^2}$*   \n",
    "\n",
    "所以优化问题转换成如下：\n",
    "$$\n",
    "\\begin{array}.\n",
    "\\max_{w\\&b}\\frac{1}{2}{\\|w\\|^2}\\\\\n",
    "s.t\\ y_i(w\\cdot{x}+b)\\geq\\hat\\gamma,i=1,2...N\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "这就是支持向量机的基本优化类型，这是一个凸二次优化问题。  \n",
    "对上述优化问题求解，得到最优解$w^*$和$b^*$，就得到了分离超平面：\n",
    "\n",
    "$$w^*x+b^*=0$$\n",
    "\n",
    "并且这个最大间隔分离超平面是存在且唯一的。详细的证明请见《统计学习方法》P101。\n",
    "\n",
    "这个问题中的$w^*$和$b^*$应该怎么求解？下面介绍拉格朗日乘子法和对偶问题。\n",
    "\n",
    "### 拉格朗日乘子法\n",
    "拉格朗日乘子法是一种寻求多元函数在约束条件下的极值的方法。通过引入拉格朗日乘子，可以将带约束条件的优化问题转换为不带约束的优化问题。\n",
    "\n",
    "这部分其实在高数部分已经学习过，给大家温习一下。\n",
    "\n",
    "有优化问题如下：\n",
    "$$\\min_{}　f(x)$$\n",
    "$$s.t\\ g(x)=0$$\n",
    "\n",
    "我们构造函数如下：\n",
    "\n",
    "$$L(x,\\lambda)=f(x)+\\lambda{g(x)}$$\n",
    "\n",
    "其中，$\\lambda$称为拉格朗日乘子。我们进行如下计算后即可求得极值：\n",
    "$$$$\n",
    "\n",
    "$$\n",
    "\\left \\{ \n",
    "\\begin{array}{ll}\n",
    "\\frac{\\partial{L(x,\\lambda)}}{\\partial{x}}=\\delta_x{f(x)}+\\lambda_x{g(x)}=0 \\\\ \n",
    "\\frac{\\partial{L(x,\\lambda)}}{\\partial{\\lambda}}={g(x)}=0\n",
    "\\end{array}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "通过计算我们就可以求得极值点$x^*$和$λ$。\n",
    "\n",
    "那这个定理是怎么来的呢？\n",
    "\n",
    "首先，由上述优化问题，我们可以得到两个结论：\n",
    "\n",
    "1.  对于约束曲面上的任意点$x$，该点的梯度$\\nabla{g(x)}$正交于约束曲面  \n",
    "2.  在最优点$x^*$，目标函数在该点的梯度$\\nabla{f(x^*)}$正交于约束曲面   \n",
    "\n",
    "这个可以比较直观的解释。\n",
    "\n",
    "想象一下，目标函数$f(x,y)$是一座山的高度，约束$g(x,y)=C$是镶嵌在山上的一条曲线如下图。\n",
    "\n",
    "![图2](https://pic1.zhimg.com/80/ecf06d062b493f1674bd81d34d3446a8_hd.png \"图2\" )\n",
    "\n",
    "你为了找到曲线上的最低点，就从最低的等高线（0那条）开始往上数。数到第三条，等高线终于和曲线有交点了（如上图所示）。因为比这条等高线低的地方都不在约束范围内，所以这肯定是这条约束曲线的最低点了。而且约束曲线在这里不可能和等高线相交，一定是相切。因为如果是相交的话，如下图所示，那么曲线一定会有一部分在$B$区域，但是$B$区域比等高线低，这是不可能的。\n",
    "\n",
    "![图3](https://pic2.zhimg.com/80/efc7e83116906f8ffebba8b0ab58dcd2_hd.jpg  \"图3\" )\n",
    "\n",
    "两条曲线相切，意味着他们在这点的法线平行，也就是法向量只差一个任意的常数乘子（取为$-\\lambda$）：$\\nabla f(x,y)=-\\lambda \\nabla g(x,y)$, 我们把这个式子的右边移到左边，并把常数移进微分算子，就得到$\\nabla (f(x,y)+\\lambda g(x,y))=0$。\n",
    "把这个式子重新解释一下，这个就是函数$f(x,y)+\\lambda g(x,y)$无约束情况下极值点的充分条件。\n",
    "\n",
    "由上述两个结论，即可得两个梯度的方向必相同或相反，即存在$λ\\neq{0}$使得：\n",
    "\n",
    "$$\\nabla(f(x^*))+\\lambda\\nabla{g(x^*)}=0$$\n",
    "\n",
    "这就得到了拉格朗日函数：\n",
    "\n",
    "$$L(x,\\lambda)=f(x)+\\lambda{g(x)}$$\n",
    "\n",
    "这就是拉格朗日乘子法的由来。\n",
    "\n",
    "### 对偶问题\n",
    "\n",
    "但有些时候，优化问题并不是那么简单，比如有如下优化问题：\n",
    "\n",
    "$$\\min_\\ \\ f(x)$$\n",
    "\n",
    "$$\n",
    "\\left.\n",
    "\\begin{array}{rr}\n",
    "s.t\\ \\ h_i=0，i=1,2\\dots,k\\\\ \n",
    "c_j\\leq0，j=1,2\\dots,l\n",
    "\\end{array}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "称此优化问题为原始问题，当有不等式优化时我们应该怎么解？\n",
    "\n",
    "我们现在需要找到优化问题的下界，即：\n",
    "\n",
    "$$f(x)\\le{v},s.t\\ \\ h_i=0,c_j(x)\\leq{0}\\ \\ (1)$$\n",
    "\n",
    "若方程$(1)$无解，则$\\ v\\ $就是原问题的一个下界，而最大的$\\ v\\ $就是问题的最优解。而问题$(1)$可以等价为：\n",
    "\n",
    "存在$λ\\leq{0}$，使得问题$(2)$无解：\n",
    "\n",
    "$$f(x)+\\sum_{i=1}^k{\\beta_ih_i(x)}+\\sum_{j=1}^l{\\lambda_jc_j(x)}<v\\ \\ (2)$$\n",
    "\n",
    "令：\n",
    "\n",
    "$$L(x,\\beta,\\lambda)=f(x)+\\sum_{i=1}^k{\\beta_ih_i(x)}+\\sum_{j=1}^l{\\lambda_jc_j(x)},\\lambda\\geq{0}\\ \\ (3)$$\n",
    "\n",
    "而方程（2）无解的充要条件是：\n",
    "\n",
    "$$\\min_{x}L(x,\\beta,\\lambda)\\geq{v}\\ \\ (4)$$\n",
    "\n",
    "\n",
    "我们希望找到最优的下界，所以：\n",
    "\n",
    "$$\\max_{\\beta,\\lambda\\geq{0}}\\min_{x}L(x,\\beta,\\lambda)\\geq{v}=v$$\n",
    "\n",
    "\n",
    "所以原问题的对偶问题是：\n",
    "\n",
    "$$\\max_{\\beta,\\lambda\\geq{0}}\\min_{x}L(x,\\beta,\\lambda)\\geq{v}$$\n",
    "\n",
    "我们求得了原始问题的对偶问题。不论原始问题的凸性如何，对偶问题一定是凸问题。我们假设原始问题的最优值为$p^*$，对偶问题的最优值为$d^*$。\n",
    "\n",
    "因为：\n",
    "\n",
    "$$\\sum_{i=1}^k{\\beta_ih_i(x)}+\\sum_{j=1}^l{\\lambda_jc_j(x)}\\leq{0}$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$L(x,\\beta,\\lambda)\\geq{v}\\leq{f(x)}$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\min_{x}L(x,\\beta,\\lambda)\\leq{v}\\leq{L(\\breve{x},\\beta,\\lambda)}\\leq{v}\\leq{f(\\breve{x})}$$\n",
    "\n",
    "其中，$\\breve{x}$为数据集中$D$中的任意点\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\min_{x}L(x,\\beta,\\lambda)\\leq{p^*}$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\max_{\\beta,\\lambda\\geq{0}}\\min_{x}L(x,\\beta,\\lambda)\\leq{p^*}$$\n",
    "\n",
    "即：\n",
    "\n",
    "$$d^*\\leq{p*}$$\n",
    "\n",
    "上述称为弱对偶性，对偶问题提供了原问题最优解的一个下界。\n",
    "\n",
    "那什么时候对偶问题的解和原始问题的解相同呢？即$d^*=p^*$呢？给出两个定理：\n",
    "\n",
    "+ **定理1**：对于原始问题和对偶问题，假设函数$f(x)$和$C_j(x)$是凸函数，$h_i(x)$是仿射函数，并且不等式约束$C_j(x)$严格成立，则存在$x^*$，$β^*$，$λ^*$，使得$x^*$是原始问题的解，$β^*$，$λ^*$是对偶问题的解，并且\n",
    "\n",
    "$$d^*=p^*=L(x^*,\\beta^*,\\lambda^*)$$\n",
    "\n",
    "+ **定理2**：对于原始问题和对偶问题，假设函数$f(x)$和$C_j(x)$是凸函数，$h_i(x)$是仿射函数，并且不等式约束$C_j(x)$严格成立，$x^*$和$β^*$、$λ^*$分别是原始问题和对偶问题的解的充分必要条件满足下面（KKT条件）：\n",
    "\n",
    "$$\n",
    "\\left\\{ \n",
    "\\begin{array}{aligh}\n",
    "\\nabla_xL(x^*,\\beta^*,\\lambda^*)=0\\\\ \n",
    "\\nabla_\\beta{L(x^*,\\beta^*,\\lambda^*)}=0\\\\\n",
    "\\nabla_\\lambda{L(x^*,\\beta^*,\\lambda^*)}=0\\\\\n",
    "{\\lambda_j}^*{c_j}(x^*)=0,j=1,2\\dots,l\\\\\n",
    "c_j(x^*)\\leq{0},j=1,2\\dots,l\\\\\n",
    "{\\lambda_j}^*\\geq0\\\\\n",
    "{h_i}^*(x^*)=0,i=1,2\\dots,k\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "通过求解上式就可以求得原始问题的最优解。将拉格朗日乘子法和对偶问题法应用到SVM中也就可以得到$w^*$和$b^*$，最终得到分离超平面：\n",
    "\n",
    "$$w^*\\cdot{x}+b^*=0$$\n",
    "\n",
    "\n",
    "\n",
    "**摘要**\n",
    "\n",
    "1. 支持向量机简介\n",
    "2. 拉格朗日乘子法\n",
    "3. 对偶问题\n",
    "\n",
    "\n",
    "支持向量机（support vector machines，SVM）是一种二分类模型，应用十分广泛，当然其也可以用于回归问题。\n",
    "\n",
    "\n",
    "\n",
    "### 第1部分\n",
    "基于最大间隔分隔数据：\n",
    "\n",
    "优点：泛化错误率低计算开销不大，结果容易解释  \n",
    "缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。  \n",
    "适用数据类型：数值型和对称型数据\n",
    "\n",
    "支持向量机(support vector machines，SVM)  是一种二分类模型，应用十分广泛，当然其也可以用于回归问题。这一节主要梳理支持向量机的基本概念、拉格朗日乘子和对偶问题。下一节主要梳理线性支持向量机、非线性支持向量机、核函数、SMO算法和支持向量回归的问题\n",
    "### 支持向量机\n",
    "支持向量机(SVM)，最主要的用途就是用作分类。如下图所示，有圆形和方形两种类别，我们希望找到一个划分超平面，将两种类别划分开\n",
    "\n",
    "![图1](https://pic4.zhimg.com/80/v2-258864cfd9b085370a8c7bc3ee602784_hd.jpg  \"图1\" )\n",
    "\n",
    "但是如右上图所示，划分超平面的方式有很多，我们应该努力寻找哪一个呢？\n",
    "\n",
    "直观上来讲的话，我们应该寻找最中间红色的那一个划分超平面，因为其对样本的鲁棒性比较好，对于未知示例的泛化能力较强。\n",
    "\n",
    "假设我们给定一个样本空间的训练集：\n",
    "\n",
    "$$D=\\{(x_1,y_1),(x_2,y_2)...(x_N,y_N)\\},x\\in R^n,y\\in\\{1,-1\\}$$\n",
    "\n",
    "其中$x$为样本特征，$y$为类别标记，${y=1}$为正例，$y=-1$为负例。\n",
    "\n",
    "我们希望找到一个分离超平面，其线性方程为：\n",
    "\n",
    "$${{w}\\cdot{x}+b=0}$$\n",
    "\n",
    "$w=\\{w_1,w_2...w_N\\}$是法向量，$b$是截距，$w\\cdot x$是内积，分离超平面也就随之确定了。\n",
    "\n",
    "\n",
    "以上就是支持向量机原理的第一部分，推导部分有点长，希望大家可以自己手动推导一遍。下次一起学习线性支持向量机、非线性支持向量机、核函数、SMO算法和支持向量回归的问题。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对偶问题  \n",
    "\n",
    "希望求解以下公式得到大间隔划分超平面所对应的模型\n",
    "\n",
    "\n",
    "$$\n",
    "f(x)=w^{T}x+b\n",
    "$$\n",
    "\n",
    "其中$ w $和$ b $是模型参数，注意到\n",
    "\n",
    "$$\n",
    "min_{w,b}  \\frac{1}{2}||w||^2\\\\\n",
    "s.t\\ \\ \\ y_i(w^{T}x_i+b)\\geq{1},i=1,2,\\dots,m\n",
    "$$\n",
    "\n",
    "本身是一个凸二次规划（convex quadratic programming）问题，能直接用现成的优化计算包求解，当我们可以有更高效的办法。\n",
    "\n",
    "\n",
    "对上式使用拉格朗日乘子法可以得到其“对偶问题”（dual problem）。具体来说，对上式的每条约束添加拉格朗日乘子$\\alpha_i\\geq{0}$则该问题的拉格朗日函数可写为\n",
    "\n",
    "$$\n",
    "L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum_{i=1}^{m}\\alpha_i(1-y_i(w^Tx_i+b))\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\alpha=(\\alpha_1;\\alpha_2;\\dots;\\alpha_m)$。令$L(w,b,\\alpha)$对$w$和$b$的偏导为零可得\n",
    "\n",
    "$$\n",
    "w=\\sum_{i=1}^{m}\\alpha_{i}{y_i}{x_i}\\\\\n",
    "0=\\sum_{i=1}^{m}\\alpha_{i}{y_i}\n",
    "$$\n",
    "\n",
    "\n",
    "代入公式即可$L(w,b,\\alpha)$中的$w$和$b$消去，再考虑约束，就可以得到对偶问题   \n",
    "\n",
    "$$\n",
    "max_{\\alpha}\\ \\ \\sum_{i=1}^{m}\\alpha_i-\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i{\\alpha_j}{y_i}{y_j}{x_i^T}{x_j}\\\\\n",
    "s.t\\ \\ \\ \\sum_{i=1}^{m}\\alpha_i{y_i}=0,\\alpha_i\\geq{0},i=1,2,\\dots,m\n",
    "$$\n",
    "\n",
    "解出$\\alpha$后，求出$w$和$b$即可得到模型\n",
    "\n",
    "$$\n",
    "f(x)=w^T{x}+b\\\\\n",
    "=\\sum_{i=1}^{m}\\alpha_i{y_i}{x_i}^T{x}+b\n",
    "$$\n",
    "\n",
    "从对偶问题解出的$\\alpha_i$是拉格朗日乘子，它恰对应着训练样本$(x_i,y_i)$，注意到式中有不等式约束，因此上述过程需满足KKT（karush-Kuhn-Tucker）条件，即要求 \n",
    "\n",
    "$$\n",
    "\\left.\\{\n",
    "\\alpha_i\\geq{0};\\\\\n",
    "y_if(x_i)-1\\geq0;\\\\\n",
    "\\alpha_i(y_if(x_i)-1)=0...\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "于是，对任意训练样本$(x_i,y_i)$，总有$\\alpha_i=0$或$y_if(x_i)=1$，若$\\alpha_i=0$，则该样本将不会再式求和中出现，也就不会对$f(x)$有任何影响；若$\\alpha_i>0$，则必有$y_if(x_i)=1$，所对应的样本位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分训练样本都不需保留，最终模型仅与支持向量有关。\n",
    "\n",
    "\n",
    "不难发现，这是一个二次规划问题，可使用通用二次规划算法来求解；然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大开销，为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效的算法，SMO（Sequential Minimal Optimization）是其中一个著名的代表。\n",
    "\n",
    "\n",
    "SMO算法的基本思路是固定$\\alpha_i$之外的所有元素，然后求$\\alpha_i$上的极值，由于存在约束$\\sum_{i=1}{m}\\alpha_i{y_i}=0$，若固定$\\alpha_i$之外的其他元素，则$\\alpha_i$可由其他变量导出，于是，SMO每次选择两个变量$\\alpha_i$和$\\alpha_j$，并规定其他参数，这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：     \n",
    "\n",
    "\n",
    "* 选取一对需更新的变量$\\alpha_i$和$\\alpha_j$;\n",
    "* 固定$\\alpha_i$和$\\alpha_j$以外参数，求解式\n",
    "$$\n",
    "max_{\\alpha}\\ \\ \\sum_{i=1}^{m}\\alpha_i-\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i{\\alpha_j}{y_i}{y_j}{x_i^T}{x_j}\\\\\n",
    "s.t\\ \\ \\ \\sum_{i=1}^{m}\\alpha_i{y_i}=0,\\alpha_i\\geq{0},i=1,2,\\dots,m\n",
    "$$\n",
    "\n",
    "获得更新后的$\\alpha_i$和$\\alpha_j$\n",
    "\n",
    ">拉格朗日乘子法  \n",
    "\n",
    "* 类别标签用-1、1，是为了后期方便 $lable*(w^Tx+b)$ 的标识和距离计算；如果 $lable*(w^Tx+b)>0$ 表示预测正确，否则预测错误。\n",
    "\n",
    "* 现在目标很明确，就是要找到w和b，因此我们必须要找到最小间隔的数据点，也就是前面所说的支持向量。\n",
    "    * 也就说，让最小的距离取最大.(最小的距离：就是最小间隔的数据点；最大：就是最大间距，为了找出最优超平面--最终就是支持向量)\n",
    "    * 目标函数：$arg: max_{关于w, b} \\left( min[lable*(w^Tx+b)]*\\frac{1}{||w||} \\right) $\n",
    "\n",
    "1. 如果 $lable*(w^Tx+b)>0$ 表示预测正确，也称函数间隔，$||w||$ 可以理解为归一化，也称几何间隔。\n",
    "\n",
    "2. 令 $lable*(w^Tx+b)>=1$， 因为0～1之间，得到的点是存在误判的可能性，所以要保障 $min[lable*(w^Tx+b)]=1$，才能更好降低噪音数据影响。\n",
    "\n",
    "3. 所以本质上是求 $arg: max_{关于w, b} \\frac{1}{||w||} $；也就说，我们约束(前提)条件是: $lable*(w^Tx+b)=1$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* 新的目标函数求解： $arg: max_{关于w, b} \\frac{1}{||w||} $\n",
    "    * => 就是求: $arg: min_{关于w, b} ||w|| $ (求矩阵会比较麻烦，如果x只是 $\\frac{1}{2}*x^2$ 的偏导数，那么。。同样是求最小值)\n",
    "    * => 就是求: $arg: min_{关于w, b} (\\frac{1}{2}*||w||^2)$ (二次函数求导，求极值，平方也方便计算)\n",
    "    * 本质上就是求线性不等式的二次优化问题(求分隔超平面，等价于求解相应的凸二次规划问题)\n",
    "\n",
    "\n",
    "* 通过拉格朗日乘子法，求二次优化问题\n",
    "    * 假设需要求极值的目标函数 (objective function) 为 f(x,y)，限制条件为 φ(x,y)=M # M=1\n",
    "    * 设g(x,y)=M-φ(x,y) # 临时φ(x,y)表示下文中 $label*(w^Tx+b)$\n",
    "    * 定义一个新函数: F(x,y,λ)=f(x,y)+λg(x,y)\n",
    "    * a为λ（a>=0），代表要引入的拉格朗日乘子(Lagrange multiplier)\n",
    "    * 那么： $L(w,b,\\alpha)=\\frac{1}{2} * ||w||^2 + \\sum_{i=1}^{n} \\alpha_i * [1 - label * (w^Tx+b)]$\n",
    "    * 因为：$label*(w^Tx+b)>=1, \\alpha>=0$ , 所以 $\\alpha*[1-label*(w^Tx+b)]<=0$ , $\\sum_{i=1}^{n} \\alpha_i * [1-label*(w^Tx+b)]<=0$\n",
    "    * 相当于求解： $max_{关于\\alpha} L(w,b,\\alpha) = \\frac{1}{2} *||w||^2$\n",
    "    * 如果求： $min_{关于w, b} \\frac{1}{2} *||w||^2$ , 也就是要求： $min_{关于w, b} \\left( max_{关于\\alpha} L(w,b,\\alpha)\\right)$\n",
    "\n",
    "\n",
    "* 现在转化到对偶问题的求解\n",
    "    * $min_{关于w, b} \\left(max_{关于\\alpha} L(w,b,\\alpha) \\right) $ >= $max_{关于\\alpha} \\left(min_{关于w, b}\\ L(w,b,\\alpha) \\right) $\n",
    "    * 现在分2步\n",
    "    * 先求： $min_{关于w, b} L(w,b,\\alpha)=\\frac{1}{2} * ||w||^2 + \\sum_{i=1}^{n} \\alpha_i * [1 - label * (w^Tx+b)]$\n",
    "    * 就是求L(w,b,a)关于[w, b]的偏导数, 得到w和b的值，并化简为：L和a的方程。\n",
    "    * 参考： 如果公式推导还是不懂，也可以参考《统计学习方法》李航-P103<学习的对偶算法>\n",
    "\n",
    "![](https://pic3.zhimg.com/80/v2-c49f7d478f4e0dd3590a6ba7a29e872e_hd.jpg)\n",
    "\n",
    "\n",
    "* 终于得到课本上的公式： $max_{关于\\alpha} \\left( \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i, j=1}^{m} label_i·label_j·\\alpha_i·\\alpha_j·<x_i, x_j> \\right) $\n",
    "* 约束条件： $a>=0$ 并且 $\\sum_{i=1}^{m} a_i·label_i=0$\n",
    "\n",
    "\n",
    ">松弛变量(slack variable)\n",
    "\n",
    "* 我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来允许数据点可以处于分隔面错误的一侧。\n",
    "* 约束条件： $C>=a>=0$ 并且 $\\sum_{i=1}^{m} a_i·label_i=0$\n",
    "* 这里常量C用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0” 这两个目标的权重。\n",
    "* 常量C是一个常数，我们通过调节该参数得到不同的结果。一旦求出了所有的alpha，那么分隔超平面就可以通过这些alpha来表示。\n",
    "* 这一结论十分直接，SVM中的主要工作就是要求解 alpha.\n",
    "\n",
    "## SMO 高效优化算法\n",
    "* SVM有很多种实现，最流行的一种实现是： 序列最小优化(Sequential Minimal Optimization, SMO)算法。\n",
    "\n",
    "* 下面还会介绍一种称为核函数(kernel)的方式将SVM扩展到更多数据集上。\n",
    "\n",
    "注意：SVM几何含义比较直观，但其算法实现较复杂，牵扯大量数学公式的推导。\n",
    "> 序列最小优化(Sequential Minimal Optimization, SMO)\n",
    "\n",
    "\n",
    "```\n",
    "创建作者：John Platt\n",
    "创建时间：1996年\n",
    "SMO用途：用于训练 SVM\n",
    "SMO目标：求出一系列 alpha 和 b,一旦求出 alpha，就很容易计算出权重向量 w 并得到分隔超平面。\n",
    "SMO思想：是将大优化问题分解为多个小优化问题来求解的。\n",
    "SMO原理：每次循环选择两个 alpha 进行优化处理，一旦找出一对合适的 alpha，那么就增大一个同时减少一个。\n",
    "这里指的合适必须要符合一定的条件\n",
    "```\n",
    "\n",
    "1. 这两个 alpha 必须要在间隔边界之外\n",
    "2. 这两个 alpha 还没有进行过区间化处理或者不在边界上。\n",
    "\n",
    "\n",
    "之所以要同时改变2个 alpha；原因是我们有一个约束条件： $\\sum_{i=1}^{m} a_i·label_i=0$；如果只是修改一个$ alpha$，很可能导致约束条件失效。\n",
    "\n",
    "\n",
    ">SMO 伪代码大致如下：\n",
    "\n",
    "\n",
    "```\n",
    "创建一个 alpha 向量并将其初始化为0向量\n",
    "当迭代次数小于最大迭代次数时(外循环)\n",
    "    对数据集中的每个数据向量(内循环)：\n",
    "        如果该数据向量可以被优化\n",
    "            随机选择另外一个数据向量\n",
    "            同时优化这两个向量\n",
    "            如果两个向量都不能被优化，退出内循环\n",
    "    如果所有向量都没被优化，增加迭代数目，继续下一次循环\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## SVM 开发流程\n",
    "```\n",
    "收集数据：可以使用任意方法。\n",
    "准备数据：需要数值型数据。\n",
    "分析数据：有助于可视化分隔超平面。\n",
    "训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。\n",
    "测试算法：十分简单的计算过程就可以实现。\n",
    "使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。\n",
    "```\n",
    "\n",
    "\n",
    "## 代码实现与注释\n",
    "\n",
    "### 1.SMO算法中辅助函数\n",
    "\n",
    "**文本格式**\n",
    "```\n",
    "3.542485\t1.977398\t-1\n",
    "3.018896\t2.556416\t-1\n",
    "7.551510\t-1.580030\t1\n",
    "2.114999\t-0.004466\t-1\n",
    "8.127113\t1.274372\t1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import operator \n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"\n",
    "    对文件进行逐行解析，从而得到第行的类标签和整个特征矩阵\n",
    "    Args:\n",
    "        fileName 文件名\n",
    "    Returns:\n",
    "        dataMat  特征矩阵\n",
    "        labelMat 类标签\n",
    "    \"\"\"\n",
    "    dataMat = [];labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]),float(lineArr[1])])\n",
    "        labelMat.append(float(lineArr[2]))\n",
    "    return dataMat, labelMat\n",
    "\n",
    "# 选择参数i之外的一个参数并返回\n",
    "def selectJrand(i,m):\n",
    "    j=i\n",
    "    while(j==i):\n",
    "        j=int(random.uniform(0,m))\n",
    "    return j\n",
    "\n",
    "# 调整aj的值\n",
    "def clipAlpha(aj,H,L):\n",
    "    if(aj>H):\n",
    "        aj=H\n",
    "    if(L>aj):\n",
    "        aj=L\n",
    "    return aj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataArr \n",
      " [[3.542485, 1.977398], [3.018896, 2.556416], [7.55151, -1.58003], [2.114999, -0.004466], [8.127113, 1.274372], [7.108772, -0.986906], [8.610639, 2.046708], [2.326297, 0.265213], [3.634009, 1.730537], [0.341367, -0.894998], [3.125951, 0.293251], [2.123252, -0.783563], [0.887835, -2.797792], [7.139979, -2.329896], [1.696414, -1.212496], [8.117032, 0.623493], [8.497162, -0.266649], [4.658191, 3.507396], [8.197181, 1.545132], [1.208047, 0.2131], [1.928486, -0.32187], [2.175808, -0.014527], [7.886608, 0.461755], [3.223038, -0.552392], [3.628502, 2.190585], [7.40786, -0.121961], [7.286357, 0.251077], [2.301095, -0.533988], [-0.232542, -0.54769], [3.457096, -0.082216], [3.023938, -0.057392], [8.015003, 0.885325], [8.991748, 0.923154], [7.916831, -1.781735], [7.616862, -0.217958], [2.450939, 0.744967], [7.270337, -2.507834], [1.749721, -0.961902], [1.803111, -0.176349], [8.804461, 3.044301], [1.231257, -0.568573], [2.074915, 1.41055], [-0.743036, -1.736103], [3.536555, 3.96496], [8.410143, 0.025606], [7.382988, -0.478764], [6.960661, -0.245353], [8.23446, 0.701868], [8.168618, -0.903835], [1.534187, -0.622492], [9.229518, 2.066088], [7.886242, 0.191813], [2.893743, -1.643468], [1.870457, -1.04042], [5.286862, -2.358286], [6.080573, 0.418886], [2.544314, 1.714165], [6.016004, -3.753712], [0.92631, -0.564359], [0.870296, -0.109952], [2.369345, 1.375695], [1.363782, -0.254082], [7.27946, -0.189572], [1.896005, 0.51508], [8.102154, -0.603875], [2.529893, 0.662657], [1.963874, -0.365233], [8.132048, 0.785914], [8.245938, 0.372366], [6.543888, 0.433164], [-0.236713, -5.766721], [8.112593, 0.295839], [9.803425, 1.495167], [1.497407, -0.552916], [1.336267, -1.632889], [9.205805, -0.58648], [1.966279, -1.840439], [8.398012, 1.584918], [7.239953, -1.764292], [7.556201, 0.241185], [9.015509, 0.345019], [8.266085, -0.230977], [8.54562, 2.788799], [9.295969, 1.346332], [2.404234, 0.570278], [2.037772, 0.021919], [1.727631, -0.453143], [1.979395, -0.050773], [8.092288, -1.372433], [1.667645, 0.239204], [9.854303, 1.365116], [7.921057, -1.327587], [8.500757, 1.492372], [1.339746, -0.291183], [3.107511, 0.758367], [2.609525, 0.902979], [3.263585, 1.367898], [2.912122, -0.202359], [1.731786, 0.589096], [2.387003, 1.573131]] \n",
      " labelArr \n",
      " [-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "dataArr,labelArr=loadDataSet(\"D:/Coding/按书籍整理程序/机器学习实战/machinelearninginaction/Ch06/testSet.txt\")\n",
    "\n",
    "print(\"dataArr \\n\",dataArr,'\\n',\"labelArr \\n\",labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.简化版SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    \"\"\"smoSimple\n",
    "\n",
    "    Args:\n",
    "        dataMatIn    特征集合\n",
    "        classLabels  类别标签\n",
    "        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。\n",
    "            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。\n",
    "            可以通过调节该参数达到不同的结果。\n",
    "        toler   容错率（是指在某个体系中能减小一些因素或选择对某个系统产生不稳定的概率。）\n",
    "        maxIter 退出前最大的循环次数\n",
    "    Returns:\n",
    "        b       模型的常量值\n",
    "        alphas  拉格朗日乘子\n",
    "    \"\"\"\n",
    "    dataMatrix = mat(dataMatIn)\n",
    "    # 矩阵转置 和 .T 一样的功能\n",
    "    labelMat = mat(classLabels).transpose()# 得到矩阵，将labelMat矩阵转置\n",
    "    \n",
    "    m, n = shape(dataMatrix)\n",
    "\n",
    "    # 初始化 b和alphas(alpha有点类似权重值。)\n",
    "    b = 0\n",
    "    alphas = mat(zeros((m, 1)))\n",
    "\n",
    "    # 没有任何alpha改变的情况下遍历数据的次数\n",
    "    iter = 0\n",
    "    while (iter < maxIter):\n",
    "        # w = calcWs(alphas, dataMatIn, classLabels)\n",
    "        # print(\"w:\", w)\n",
    "\n",
    "        # 记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            # print 'alphas=', alphas\n",
    "            # print 'labelMat=', labelMat\n",
    "            # print 'multiply(alphas, labelMat)=', multiply(alphas, labelMat)\n",
    "            # 我们预测的类别 y[i] = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*lable[n]*x[n]\n",
    "            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b\n",
    "            # 预测结果与真实结果比对，计算误差Ei\n",
    "            Ei = fXi - float(labelMat[i])\n",
    "\n",
    "            # 约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值)\n",
    "            # 0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。\n",
    "            # 表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。\n",
    "            '''\n",
    "            # 检验训练样本(xi, yi)是否满足KKT条件\n",
    "            yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n",
    "            yi*f(i) == 1 and 0<alpha< C (on the boundary)\n",
    "            yi*f(i) <= 1 and alpha = C (between the boundary)\n",
    "            '''\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "\n",
    "                # 如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较\n",
    "                j = selectJrand(i, m)\n",
    "                # 预测j的结果\n",
    "                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                alphaIold = alphas[i].copy()\n",
    "                alphaJold = alphas[j].copy()\n",
    "\n",
    "                # L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句\n",
    "                # labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                # 如果相同，就没发优化了\n",
    "                if L == H:\n",
    "                    print(\"L==H\")\n",
    "                    continue\n",
    "\n",
    "                # eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程\n",
    "                # 参考《统计学习方法》李航-P125~P128<序列最小最优化算法>\n",
    "                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T\n",
    "                if eta >= 0:\n",
    "                    print(\"eta>=0\")\n",
    "                    continue\n",
    "\n",
    "                # 计算出一个新的alphas[j]值\n",
    "                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n",
    "                # 并使用辅助函数，以及L和H对其进行调整\n",
    "                alphas[j] = clipAlpha(alphas[j], H, L)\n",
    "                # 检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。\n",
    "                if (abs(alphas[j] - alphaJold) < 0.00001):\n",
    "                    print(\"j not moving enough\")\n",
    "                    continue\n",
    "                # 然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反\n",
    "                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\n",
    "                # 在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。\n",
    "                # w= Σ[1~n] ai*yi*xi => b = yj- Σ[1~n] ai*yi(xi*xj)\n",
    "                # 所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)\n",
    "                # 为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍\n",
    "                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T\n",
    "                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T\n",
    "                if (0 < alphas[i]) and (C > alphas[i]):\n",
    "                    b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]):\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2)/2.0\n",
    "                alphaPairsChanged += 1\n",
    "                print(\"iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n",
    "        # 在for循环外，检查alpha值是否做了更新，如果在更新则将iter设为0后继续运行程序\n",
    "        # 知道更新完毕后，iter次循环无变化，才推出循环。\n",
    "        if (alphaPairsChanged == 0):\n",
    "            iter += 1\n",
    "        else:\n",
    "            iter = 0\n",
    "        print(\"iteration number: %d\" % iter)\n",
    "    return b, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.完整版SMO支持函数\n",
    "\n",
    "程序清单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整版SMO支持函数\n",
    "class optStruct:\n",
    "    def __init__(self,dataMatIn,classLabels,C,toler,Ktup):\n",
    "        self.X=dataMatIn\n",
    "        self.labelMat=classLabels\n",
    "        self.C=C\n",
    "        self.m=shape(dataMatIn)[0]\n",
    "        self.alphas=mat(zeros((self.m,1)))\n",
    "        self.b=0\n",
    "        # eCache中第一列为标志位，第二列是误差E值\n",
    "        self.eCache=mat(zeros((self.m,2)))\n",
    "        self.tol=toler\n",
    "        self.K=mat(zeros(self.m,self.m))\n",
    "        for i in range(self.m):\n",
    "            self.K[:,i]=kernelTrans(self.X,self.X[i,:],kup)\n",
    "\n",
    "def calcEk(oS,k):\n",
    "    # 计算预测值\n",
    "    fXk=float(multiply(oS.alphas,oS.labelMat).T*\n",
    "              (oS.X*oS.X[k,:].T))+oS.b\n",
    "    # 计算误差\n",
    "    Ek=fXk-float(oS.labelMat[k])\n",
    "    return Ek\n",
    "\n",
    "# 选择第二个alpha\n",
    "def selectJ(i,oS,Ei):\n",
    "    maxK=-1;maxDeltaE=0;Ej=0\n",
    "    # 将输入值在eCache中设置为有效\n",
    "    oS.eCache[i]=[1,Ei]\n",
    "    # 返回非零E值的位置\n",
    "    validEcacheList=nonzero(oS.eCache[:,0].A)[0]\n",
    "    if((len(validEcacheList))>1):\n",
    "        # 选择误差E最大的\n",
    "        for k in validEcacheList:\n",
    "            if k==i:continue\n",
    "            Ek=calcEk(oS,k)\n",
    "            deltaE=abs(Ei-Ek)\n",
    "            if(deltaE>maxDeltaE):\n",
    "                maxK=k\n",
    "                maxDeltaE=deltaE\n",
    "                Ej=Ek\n",
    "        return maxK,Ej\n",
    "    # 如果是第一次循环，就随机挑选一个j\n",
    "    else:\n",
    "        j=selectJrand(i,oS.m)\n",
    "        Ej=calcEk(oS,j)\n",
    "    return j,Ej\n",
    "\n",
    "# 更新eCache中的误差值\n",
    "def updateEk(oS,k):\n",
    "    Ek=calcEk(oS,k)\n",
    "    oS.eCache[k]=[1,Ek]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.完整的SMO优化例程\n",
    "\n",
    "程序清单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的SMO的优化例程\n",
    "# 其和上面简化版的基本一致，公式部分看不懂看正文\n",
    "def innerL(i,oS):\n",
    "    Ei=calcEk(oS,i)\n",
    "    if((oS.labelMat[i]*Ei<-oS.tol)and(oS.alphas[i]<oS.C)or\n",
    "        (oS.labelMat[i] * Ei>oS.tol) and (oS.alphas[i] >0)):\n",
    "        j,Ej=selectJ(i,oS,Ei)\n",
    "        alphaIold=oS.alphas[i].copy()\n",
    "        alphaJold=oS.alphas[j].copy()\n",
    "        if(oS.labelMat[i]!=oS.labelMat[j]):\n",
    "            L=max(0,oS.alphas[j]-oS.alphas[i])\n",
    "            H=min(oS.C,oS.C+oS.alphas[j]-oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i]-oS.C)\n",
    "            H = min(oS.C,oS.alphas[j] + oS.alphas[i])\n",
    "        if(L==H):print('L==H');return 0\n",
    "        eta=(2.0*oS.X[i,:]*oS.X[j,:].T-oS.X[i,:]*oS.X[i,:].T-\n",
    "             oS.X[j,:]*oS.X[j,:].T)\n",
    "        if(eta>=0):print(\"eta>=0\");return 0\n",
    "        oS.alphas[j]-=oS.labelMat[j]*(Ei-Ej)/eta\n",
    "        oS.alphas[j]=clipAlpha(oS.alphas[j],H,L)\n",
    "        updateEk(oS,j)\n",
    "        if((abs(oS.alphas[j]-alphaJold)<0.00001)):\n",
    "            print('j not moving enough')\n",
    "            return 0\n",
    "        oS.alphas[i]+=oS.labelMat[j]*oS.labelMat[i]*(alphaJold-oS.alphas[j])\n",
    "        updateEk(oS,i)\n",
    "        b1=(oS.b-Ei-oS.labelMat[i]*(oS.alphas[i]-alphaIold)*\n",
    "            oS.X[i,:]*oS.X[i,:].T-oS.labelMat[j]*\n",
    "            (oS.alphas[j]-alphaJold)*oS.X[i,:]*oS[j,:].T)\n",
    "        b2 = (oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) *\n",
    "              oS.X[i, :] * oS.X[j, :].T - oS.labelMat[j] *\n",
    "              (oS.alphas[j] - alphaJold) * oS.X[j, :] * oS[j, :].T)\n",
    "        if((0<oS.alphas[i])and(oS.C>oS.alphas[i])):\n",
    "            oS.b=b1\n",
    "        elif((0<oS.alphas[j])and(oS.C>oS.alphas[j])):oS.b=b2\n",
    "        else:oS.b=(b1+b2)/2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 完整版SMO的外循环代码\n",
    "def smoP(dataMatIn,classLabels,C,toler,maxIter,kTup=('lin',0)):\n",
    "    oS.optStruct(mat(dataMatin),mat(classLabels).transpose(),C,toler)\n",
    "    iter=0\n",
    "    entireSet=True;alphaPairsChanged=0\n",
    "    while((iter<maxIter)and((alphaPairsChanged>0)or(entireSet))):\n",
    "        alphaPairsChanged=0\n",
    "        if(entireSet):\n",
    "            for i in range(oS.m):\n",
    "                alphaPairsChanged+=innerL(i,oS)\n",
    "                print('fullSet iter : %d i:%d,pairs changed %d'%\n",
    "                      (iter,i,alphaPairsChanged))\n",
    "                iter+=1\n",
    "        else:\n",
    "            nonBoundIs=nonzero((oS.alphas.A>0)*(oS.alphas.A<C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged+=innerL(i,oS)\n",
    "                print(\"non-bound,iter:%d i:%d,pairs changed %d\"%\n",
    "                      (iter,i,alphaPairsChanged))\n",
    "                iter+=1\n",
    "        if(entireSet):\n",
    "            entireSet=False\n",
    "        elif(alphaPairsChanged==0):\n",
    "            entireSet=True\n",
    "        print('iteration number: %d'%iter)\n",
    "    return oS.b,oS.alphas\n",
    "\n",
    "def calWs(alphas,dataArr,classLabels):\n",
    "    X=mat(dataArr);labelMat=mat(classLabels).transpose()\n",
    "    m.n=shape(X)\n",
    "    w=zeros((n,1))\n",
    "    for i in range(m):\n",
    "        w+=multiply(alphas[i]*labelMat[i],X[i,:].T)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 第2部分\n",
    "\n",
    "**摘要**\n",
    "\n",
    "1. 线性支持向量机  \n",
    "2. 非线性支持向量机和核函数  \n",
    "3. SMO算法  \n",
    "4. 支持向量回归  \n",
    "5. 代码实现与注释\n",
    "\n",
    "### 线性支持向量机\n",
    "\n",
    "**1.线性可分支持向量机**\n",
    "线性可分支持向量机是SVM中最简单最基本的形式，如下图:\n",
    "\n",
    "![图5](https://pic4.zhimg.com/80/v2-3481b8c65c5ab920de646e43db72e4a0_hd.jpg \"图5\")\n",
    "\n",
    "即通过一个超平面就可以完全将两个类别分开。\n",
    "\n",
    "这种线性可分支持向量机的基本型：\n",
    "\n",
    "$$\n",
    "min \\frac{1}{2}||w||^2\\\\\n",
    "s.t y_i(w·x_i+b)-1\\geq{0},i=1,2,\\dots,N\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
