{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#预测数值型数据：回归\" data-toc-modified-id=\"预测数值型数据：回归-1\">预测数值型数据：回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#用线性回归找到最佳拟合曲线\" data-toc-modified-id=\"用线性回归找到最佳拟合曲线-1.1\">用线性回归找到最佳拟合曲线</a></span></li><li><span><a href=\"#回归的一般方法\" data-toc-modified-id=\"回归的一般方法-1.2\">回归的一般方法</a></span></li><li><span><a href=\"#线性回归\" data-toc-modified-id=\"线性回归-1.3\">线性回归</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测数值型数据：回归\n",
    "**摘要**\n",
    "\n",
    "1.线性回归\n",
    "\n",
    "2.局部加权线性回归\n",
    "\n",
    "3.岭回归和逐步线性回归\n",
    "\n",
    "4.预测鲍鱼年龄和玩具售价代码实现和注释\n",
    "\n",
    "\n",
    "\n",
    "## 用线性回归找到最佳拟合曲线\n",
    "\n",
    "**线性回归**\n",
    "> 优点：结果易于理解，计算上不复杂。  \n",
    "缺点：对非线性的数据拟合不好。    \n",
    "适用于数据类型：数值型和标称型数据。   \n",
    "\n",
    "\n",
    "**回归没特别说明的话等同于线性回归**\n",
    "\n",
    "## 回归的一般方法\n",
    ">收集数据: 采用任意方法收集数据   \n",
    "准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据   \n",
    "分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比   \n",
    "训练算法: 找到回归系数    \n",
    "测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果   \n",
    "使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签  \n",
    "\n",
    "\n",
    "## 线性回归  \n",
    "\n",
    "我们给定数据集 $D=\\left\\{ (x_1,y_1),(x_2,y_2),...(x_m,y_m) \\right\\} $，其中$ x_i=(x_{i1},x_{i2},...x_{id}) $，$ y_i\\in R$ .\n",
    "\n",
    "线性回归就是想学得一个线性模型来尽可能地预测真实值，即：\n",
    "$$\n",
    "f(x)=w_1x_1+w_2x_2+...+w_dx_d+d\n",
    "$$\n",
    "写成向量的形式就是：\n",
    "$$\n",
    "f(x)=w^Tx+b\n",
    "$$\n",
    "其中 $w=(w_1;w_2;w_3...w_d)$ ，当我们求得$w$和$d$之后，模型也就确定了。\n",
    "\n",
    "我们如何确定$w$和$b$呢？我们需要给定一个目标函数。一般情况下我们采用均方误差最小化，即：\n",
    "$$\n",
    "(w^*,b^*)=argmin\\sum_{i=1}^{m}{(f(x_i)-y_i)^2}\n",
    "$$\n",
    "为了便于讨论，我们把w和x写成如下形式：\n",
    "$$\n",
    "\\begin{array}{aligh}\n",
    "\\tilde{w}=(w;b)\\\\\n",
    "x_i=(x_{i1},x_{i2},...x_{id},1)\n",
    "\\end{array}\n",
    "$$\n",
    "这样，线性回归模型就可以写为：\n",
    "$$\n",
    "f(x)=\\tilde{w}^T\\cdot x\n",
    "$$\n",
    "那上面均方误差最小化写成向量的形式就等价于：\n",
    "$$\n",
    "\\tilde{w}=argmin(y-X\\tilde{w})^T(y-X\\tilde{w})\n",
    "$$\n",
    "其中，$X$为：\n",
    "$$\n",
    "X=\\left( \\begin{array}{ccc} x_{11} & x_{12} & ...&x_{1d}&1 \\\\ x_{21} & x_{22} & ...&x_{2d}&1 \\\\ ... & ... & ...&...&... \\\\ x_{m1} & x_{m2} & ...&x_{md}&1 \\end{array} \\right)\n",
    "$$\n",
    "令 $E_\\tilde{w}=(y-X\\tilde{w})^T(y-X\\tilde{w})$ ，我们令其对 $\\tilde{w}$ 求导：\n",
    "$$\n",
    "\\frac{dE_{\\tilde{w}}}{d\\tilde{w}}=\\frac{d(y-X\\tilde{w})^T(y-X\\tilde{w})}{d\\tilde{w}}=2X^T(X\\tilde{w}-y)\n",
    "$$\n",
    "这个矩阵求导结果怎么得到的呢？很多地方都只是给了结果。\n",
    "\n",
    "推导这一部分得有一些线性代数的知识，关于机器学习数学方面的内容，后面有时间的话我会系统梳理。这里先给大家简单推导一遍，想要详细了解的同学可以看这篇文章，写得很好：https://zhuanlan.zhihu.com/p/24709748\n",
    "\n",
    "我假设大家已经看了上面的文章，了解了具体的公式。\n",
    "\n",
    "由 $d(XY)=dXY+XdY$ ，得到  \n",
    "$\n",
    "=\\cfrac{d(y-X\\tilde{w})^T(y-X\\tilde{w})}{d\\tilde{w}}\n",
    "$\n",
    "\n",
    "$\n",
    "=(Xd\\tilde{w})^T(y-X\\tilde{w})+(y-X\\tilde{w})^T(Xd\\tilde{w})\n",
    "$\n",
    "\n",
    "再由 $df=tr(\\cfrac{\\alpha f}{\\alpha X}^TdX)$ ，得到   \n",
    "\n",
    "$\n",
    "=tr((Xd\\tilde{w})^T(y-X\\tilde{w})+(y-X\\tilde{w})^T(Xd\\tilde{w}))\n",
    "$    \n",
    "\n",
    "再由 $tr(A\\pm B)=tr(A)\\pm tr(B)$ ，得到   \n",
    "\n",
    "$\n",
    "=tr((Xd\\tilde{w})^T(y-X\\tilde{w}))+tr((y-X\\tilde{w})^T(Xd\\tilde{w}))\n",
    "$\n",
    "\n",
    "因为 $tr(A^T)=tr(A)$ ，所以\n",
    "\n",
    "$\n",
    "=tr(Xd\\tilde{w}(y-X\\tilde{w})^T)+tr((y-X\\tilde{w})^T(Xd\\tilde{w}))\n",
    "$\n",
    "\n",
    "又因为 $tr(AB)=tr(BA)$ ，所以\n",
    "\n",
    "$\n",
    "=tr((y-X\\tilde{w})^TXd\\tilde{w})+tr((y-X\\tilde{w})^T(Xd\\tilde{w}))\n",
    "$\n",
    "\n",
    "$\n",
    "=tr(2(y-X\\tilde{w})^TXd\\tilde{w})\n",
    "$\n",
    "\n",
    "$\n",
    "=tr(\\cfrac{\\alpha f}{\\alpha \\tilde{w}}^Td\\tilde{w})\n",
    "$\n",
    "\n",
    "所以得到：   \n",
    "$\n",
    "\\cfrac{\\alpha f}{\\alpha \\tilde{w}}=2X^T(y-X\\tilde{w})\n",
    "$\n",
    "\n",
    "令上式等于零，得到 $\\tilde{w}$ 最优解的闭式解：\n",
    "$$\n",
    "\\tilde{w}^*=(X^T X)^{-1}X^Ty\n",
    "$$\n",
    "当然上式成立的前提条件是 $X^T X$ 可逆。在得到 $\\tilde{w}^*$ 之后，我们也就可以写出线性回归模型：\n",
    "$$\n",
    "f(x)=(\\tilde{w}^*)^Tx\n",
    "$$\n",
    "以上就是线性回归模型的基本形式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
