{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#机器学习实战——AdaBoost算法\" data-toc-modified-id=\"机器学习实战——AdaBoost算法-1\">机器学习实战——AdaBoost算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#bagging：基于数据随机重抽样的分类器的构建方法\" data-toc-modified-id=\"bagging：基于数据随机重抽样的分类器的构建方法-1.1\">bagging：基于数据随机重抽样的分类器的构建方法</a></span></li><li><span><a href=\"#boosting\" data-toc-modified-id=\"boosting-1.2\">boosting</a></span></li><li><span><a href=\"#集成方法:-ensemble-method（元算法:-meta-algorithm）-概述\" data-toc-modified-id=\"集成方法:-ensemble-method（元算法:-meta-algorithm）-概述-1.3\">集成方法: ensemble method（元算法: meta algorithm） 概述</a></span></li><li><span><a href=\"#集成方法-场景\" data-toc-modified-id=\"集成方法-场景-1.4\">集成方法 场景</a></span></li><li><span><a href=\"#随机森林\" data-toc-modified-id=\"随机森林-1.5\">随机森林</a></span><ul class=\"toc-item\"><li><span><a href=\"#随机森林-概述\" data-toc-modified-id=\"随机森林-概述-1.5.1\">随机森林 概述</a></span></li></ul></li><li><span><a href=\"#随机森林-原理\" data-toc-modified-id=\"随机森林-原理-1.6\">随机森林 原理</a></span><ul class=\"toc-item\"><li><span><a href=\"#训练算法：基于错误提升分类器的性能\" data-toc-modified-id=\"训练算法：基于错误提升分类器的性能-1.6.1\">训练算法：基于错误提升分类器的性能</a></span></li></ul></li><li><span><a href=\"#代码实现与注释\" data-toc-modified-id=\"代码实现与注释-1.7\">代码实现与注释</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.单层决策树生成函数\" data-toc-modified-id=\"1.单层决策树生成函数-1.7.1\">1.单层决策树生成函数</a></span></li></ul></li><li><span><a href=\"#Adaboost算法的误差\" data-toc-modified-id=\"Adaboost算法的误差-1.8\">Adaboost算法的误差</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 机器学习实战——AdaBoost算法\n",
    "\n",
    "**本章内容**\n",
    ">1. 组合相似的分类器来提高分类性能   \n",
    "2. 应用AdaBoost算法   \n",
    "3. 处理非均衡分类问题   \n",
    "\n",
    "前面已经介绍了五种不同的分类算法，它们各有优缺点。将不同的分类器组合起来，这种组合结果称为*集成方法（ensemble method）*或者*元算法（meta-algorithm）*，使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。\n",
    "\n",
    "AdaBoost\n",
    "> 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整  \n",
    "缺点：对离群点敏感  \n",
    "适用数据类型：数值型和标称型  \n",
    "\n",
    "## bagging：基于数据随机重抽样的分类器的构建方法\n",
    "\n",
    "　**自举汇聚法（bootstrap aggregating）**，也称为bagging方法，是在从原始数据集选择$S$次后得到$S$个新数据集的一种技术。新数据集和原数据集的大小相等。每个数据集都是通过在原始数据集中随机选择一个样本来进行替换得到的。这里的替换就意味着可以多次地选择同一个样本。这一性质就允许新数据集中可以有重复地值，而原始数据集的某些值在新集合中则不再出现。 \n",
    "\n",
    "　在$S$个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了$S$个分类器。当我们要对新数据进行分类时，就可以应用则$S$个分类器进行分类。与此同时，选择分类器投票结果中最多的类别作为最后的分类结果。 \n",
    "\n",
    "　还有更先进的bagging方法，比如随机森林（random forest）\n",
    "\n",
    "## boosting\n",
    "\n",
    "　boosting是一种与bagging很类似的技术。无论是在boosting还是在bagging中，所使用的多个分类器的类型都是相同的。但是在前者中，不同的分类器是通过串行训练而获得的，每个新分类器都是根据已训练出的分类器的性能来进行训练的。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。  \n",
    "\n",
    "　由于boosting分类的结果是基于所有分类器的加权求和结果的，因此boosting与bagging不大一样。bagging中的分类器权重是相等的，而bagging中的分类器权重是相等的，而boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。\n",
    " \n",
    "boosting方法拥有很多版本，本站只关注其中一个最流行的版本Adaboost。\n",
    " \n",
    "\n",
    "## 集成方法: ensemble method（元算法: meta algorithm） 概述\n",
    "* 概念：是对其他算法进行组合的一种形式。\n",
    "* 通俗来说： 当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。 机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。\n",
    "* 集成方法：\n",
    "    1. 投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法\n",
    "    2. 再学习(boosting): 是基于所有分类器的加权求和的方法\n",
    "\n",
    "\n",
    "## 集成方法 场景\n",
    "\n",
    "　目前 bagging 方法最流行的版本是: 随机森林(random forest)\n",
    "选男友：美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友\n",
    "\n",
    "目前 boosting 方法最流行的版本是: AdaBoost\n",
    "追女友：3个帅哥追同一个美女，第1个帅哥失败->(传授经验：姓名、家庭情况) 第2个帅哥失败->(传授经验：兴趣爱好、性格特点) 第3个帅哥成功\n",
    "\n",
    ">bagging 和 boosting 区别是什么？  \n",
    "\n",
    "1. bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。\n",
    "2. bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。\n",
    "3. bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。\n",
    "\n",
    "\n",
    "## 随机森林\n",
    "\n",
    "### 随机森林 概述\n",
    "\n",
    "* 随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。\n",
    "* 决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。\n",
    "\n",
    "## 随机森林 原理\n",
    "\n",
    "那随机森林具体如何构建呢？\n",
    "有两个方面：\n",
    "\n",
    "1. 数据的随机性化\n",
    "2. 待选特征的随机化\n",
    "\n",
    "使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。\n",
    "\n",
    ">数据的随机化：使得随机森林中的决策树更普遍化一点，适合更多的场景。\n",
    "（有放回的准确率在：70% 以上， 无放回的准确率在：60% 以上）\n",
    "\n",
    "1. 采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）\n",
    "2. 利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。\n",
    "3. 然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果。\n",
    "如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。\n",
    "\n",
    "![](https://pic2.zhimg.com/80/v2-7e1aefdea2a93256dbf474e25f8889a0_hd.jpg)\n",
    "\n",
    ">待选特征的随机化\n",
    "\n",
    "1. 子树从所有的待选特征中随机选取一定的特征。\n",
    "2. 在选取的特征中选取最优的特征。\n",
    "下图中，蓝色的方块代表所有可以被选择的特征，也就是目前的待选特征；黄色的方块是分裂特征。\n",
    "左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（别忘了前文提到的ID3算法，C4.5算法，CART算法等等），完成分裂。\n",
    "右边是一个随机森林中的子树的特征选取过程。\n",
    "\n",
    "![](https://pic2.zhimg.com/80/v2-2414181004d297b27e135c46a42d0119_hd.jpg)\n",
    "\n",
    "AdaBoost的一般流程\n",
    ">(1)收集数据：可以用任意方法。   \n",
    "(2)准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器，第2章到第6章中的任一分类器都可以充当弱分类器。作为弱分类器，简单分类器的效果更好。    \n",
    "(3)分析数据：可以使用任意方法。  \n",
    "(4)训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。  \n",
    "(5)测试算法：计算分类的错误率。   \n",
    "(6)使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类别场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改。  \n",
    "\n",
    "\n",
    "### 训练算法：基于错误提升分类器的性能\n",
    "\n",
    "能否使用弱分类器和多个实例来构建一个强分类器？\n",
    "这里“弱”分类器意味着分类器的性能比随机猜测要略好，但是也不是好太多。即，在二分类的情况下弱分类器的错误率会高于50%，而“强”分类器的错误率将会低很多。AdaBoost算法即脱胎于上述理论问题。\n",
    "\n",
    "\n",
    "AdaBoost是adaptive boosting（自适应boosting）的缩写，\n",
    "其运行过程如下：\n",
    ">训练数据汇总的每个样本，并赋予其一个权重，这些权重构成向量$D$。一开始，这些权重都初始化成相等的值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后再同一个数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。其中错误率$\\epsilon$的定义为：\n",
    "$$\n",
    "\\epsilon=\\frac{未正确分类的样本数目}{所有样本数目}\n",
    "$$\n",
    "而alpha的计算公式如下：\n",
    "$$\n",
    "\\alpha=\\frac{1}{2}\\ln \\big(\\frac{1-\\epsilon}{\\epsilon}\\big)\n",
    "$$\n",
    "AdaBoost算法流程如下：\n",
    "\n",
    "计算出alpha值后，可以对权重向量$D$进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。$D$的计算方法如下。\n",
    "如果某个样本被正确分类，那么该样本的权证更改为：\n",
    "$$\n",
    "D_{i}^{t+1}=\\frac{D_{i}^{t}e^{-\\alpha}}{SUM(D)}\n",
    "$$\n",
    "而如果某个样本被错分，那么该样本的权重更改为：\n",
    "$$\n",
    "D_{i}^{t+1}=\\frac{D_{i}^{t}e^{\\alpha}}{SUM(D)}\n",
    "$$\n",
    "在计算出$D$之后，AdaBoost又开始进入下一轮迭代。会不断重复训练和调整权重过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。\n",
    "\n",
    "## 代码实现与注释\n",
    "\n",
    "在下面的程序中，基分类器选用的是决策树，但是基分类器可以选取我们所知道的任意一个分类器都可以，比如神经网络等。\n",
    "\n",
    "### 1.单层决策树生成函数\n",
    "\n",
    "程序清单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# 加载数据\n",
    "def loadsimpData():\n",
    "    dataMat=matrix([[1,2.1],\n",
    "                    [2,1.1],\n",
    "                    [1.3,1],\n",
    "                    [1,1],\n",
    "                    [2,1]])\n",
    "    classLabels=[1,1,-1,-1,1]\n",
    "    return dataMat,classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1. , 2.1],\n",
       "         [2. , 1.1],\n",
       "         [1.3, 1. ],\n",
       "         [1. , 1. ],\n",
       "         [2. , 1. ]]), [1, 1, -1, -1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datMat,classLabel=loadsimpData()\n",
    "datMat,classLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序的伪代码如下：\n",
    "`\n",
    "将最小错误率minError设为+$\\infty$   \n",
    "对数据集中的每个特征（第一层循环）：   \n",
    "　　对每个步长（第二层循环）：   \n",
    "　　　　对每个不等号（第三层循环）：   \n",
    "　　　　　　建立一棵单层决策树并利用加权数据集对它进行测试    \n",
    "　　　　　　如果错误率低于minError，则将当前单层决策树设为最佳决策树    \n",
    "返回最佳单层决策树               \n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-431e77c778d7>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-431e77c778d7>\"\u001b[1;36m, line \u001b[1;32m41\u001b[0m\n\u001b[1;33m    inequal,weightedError))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 单层决策树生成函数\n",
    "# dimen是哪一个特征；threshVal是特征阈值；threshIneq是大于还是小于\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    # 初始化一个全1列表\n",
    "    retArray=ones((shape(dataMatrix)[0],1))\n",
    "    if(threshIneq=='lt'):\n",
    "        # 以阈值划分后，小于等于阈值的，类别定为-1\n",
    "        retArray[dataMatrix[:,dimen]<=threshVal]=-1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen]>threshVal]=-1.0\n",
    "    return retArray\n",
    "\n",
    "# D是权重向量\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    dataMatrix=mat(dataArr);labelMat=mat(classLabels).T\n",
    "    m,n=shape(dataMatrix)\n",
    "    numSteps=10.0;bestStump={};bestClassEst=mat(zeros((m,1)))\n",
    "    # 最小值初始化为无穷大\n",
    "    minError=inf\n",
    "    # 对每一个特征\n",
    "    for i in range(n):\n",
    "        # 找到最大值和最小值\n",
    "        rangeMin=dataMatrix[:,i].min()\n",
    "        rangeMax=dataMatrix[:,i].max()\n",
    "        # 确定步长\n",
    "        stepSize=(rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                # 得到阈值\n",
    "                threshVal=(rangeMin+float(j)*stepSize)\n",
    "                # 调用函数，并得到分类列表\n",
    "                predictedVals=stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                # 初始化errArr\n",
    "                errArr=mat(ones((m,1)))\n",
    "                # 将errArr中分类正确的置为0\n",
    "                errArr[predictedVals==labelMat]=0\n",
    "                # 计算加权错误率\n",
    "                weightedError=D.T*errArr\n",
    "                # print(\"split:dim %d,thresh %.2f,thresh inequal:\"\n",
    "                       # \"%s,the weighted error is %.3f\"%(i,threshVal,\n",
    "                          inequal,weightedError))\n",
    "                # 如果错误率比之前的小\n",
    "                if(weightedError<minError):\n",
    "                    minError=weightedError\n",
    "                    # bestClassEst中是错误最小的分类类别\n",
    "                    bestClassEst=predictedVals.copy()\n",
    "                    bestStump['dim']=i\n",
    "                    bestStump['thresh']=threshVal\n",
    "                    bestStump['ineq']=inequal\n",
    "    return bestStump,minError,bestClassEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**摘要**\n",
    "\n",
    "1. AdaBoost元算法  \n",
    "\n",
    "2. AdaBoost元算法误差分析  \n",
    "\n",
    "3. 代码实现与解释  \n",
    "\n",
    "**AdaBoost算法**\n",
    "\n",
    "AdaBoost算法是集成学习的一种。\n",
    "\n",
    "集成学习就是构建多个“基学习器”，之后再将它们结合来完成学习任务的方法。集成学习通过将多个学习器进行综合，其性能通常比单个学习器要好。我们之前提过的随机森林就是集成学习的一种。\n",
    "\n",
    "Adaboost算法：先从初始训练集合中训练出一个基学习器，再根据基学习器的表现对训练样本的权重进行调整，使得先前基学习器做错的样本在后续得到更多的关注，然后基于调整后的样本权重来训练下一个基学习器，直到基学习器的数目达到事先指定的数目$M$，最终将这$M$个学习器进行加权组合。\n",
    "\n",
    "首先我们假设给定一个二分类的训练数据集：\n",
    "\n",
    "$$T=\\{(x_1,y_1),(x_2,y_2)\\dots(x_N,y_N)\\}$$\n",
    "\n",
    "其中，$x\\in{R^n}$,$y\\in\\{-1,+1\\}$\n",
    "\n",
    "初始化样本的权重为：\n",
    "\n",
    "$$D_1=\\{w_{11},w_{12},\\dots{w_{1N}}\\},w_{1i}=\\frac{1}{N}$$\n",
    "\n",
    "第m个基分类器的样本权重为：\n",
    "\n",
    "$$D_m=\\{w_{m1},w_{m2},\\dots{w_{mN}}\\},\\sum_{i=1}^N{w_{mi}}=1$$\n",
    "\n",
    "我们构建$M$个基学习器，最终的学习器即为基学习器的线性组合：\n",
    "\n",
    "$$f(x)=\\sum_{i=1}^M{\\alpha{G_i(x)}}$$\n",
    "\n",
    "\n",
    "其中$\\ \\alpha_{i}$为第$\\ i$个基学习器的系数，$G_{i}(x)$为第$\\ i$个基学习器。\n",
    "\n",
    "$$e_m=\\sum_{i=1}^{N}{P(G_m(x_i)\\ne y_i)}=\\sum_{i=1}^{N}{w_{mi}I(G_m(x_i) \\ne y_i)} $$\n",
    "\n",
    "也就是说$G_m(x)$在加权训练数据集中的分类误差率为被误分类的样本权值之和。注意一下，我们定义的基学习器，其$e_m<0.5$。\n",
    "\n",
    "接着我们定义损失函数为指数损失函数：\n",
    "\n",
    "$$L(y,f(x))=E_{x\\sim D}\\left[ exp(-yf(x)) \\right] $$\n",
    "\n",
    "其中$y$是样本的实际类别，$f(x)$是预测的类别，样本$x$的权重服从$D$分布。$E$代表求期望。\n",
    "\n",
    "损失函数为什么这样定义？下面一起证明一下：\n",
    "\n",
    "若$f(x)$能使损失函数最小化，那我们考虑上式对$f(x)$的偏导为零：\n",
    "\n",
    "\n",
    "$$\\frac{\\alpha L(y,f(x))}{\\alpha f(x)}=-e^{-f(x)}P(y=1|x)+e^{f(x)}P(y=-1|x)$$ \n",
    "\n",
    "令上式为零，得：\n",
    "\n",
    "$$f(x)=\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)}$$\n",
    "\n",
    "因此，有\n",
    "\n",
    "$$sign(f(x))=sign(\\frac{1}{2}ln\\frac{P(y=1|x)}{P(y=-1|x)})$$ \n",
    "\n",
    "当$P(y=1|x)>P(y=-1|x)$时，$sign(f(x))=1$\n",
    "\n",
    "当$P(y=1|x)>P(y=-1|x)$时，$sign(f(x))=-1$\n",
    "\n",
    "这样的分类规则正是我们所需要的，若指数函数最小化，则分类错误率也最小，它们俩是一致的。所以我们的损失函数可以这样定义。\n",
    "\n",
    "定义完了损失函数，我们看怎么来进行基分类器$G_m(x)$和系数$\\alpha_{i}$的求取。\n",
    "\n",
    "第一个分类器$G1(x)$是直接将基学习算法用于初始数据分布求得，之后不断迭代，生成$\\alpha_m$和$G_m$。当第$m$个基分类器产生后,我们应该使得其在数据集第$m$轮样本权重基础上的指数损失最小，即\n",
    "\n",
    "\n",
    "\\begin{aligned} % requires amsmath; align* for no eq. number\n",
    "L(\\alpha_m,G_m(x)) & =argmin  E_{x\\sim D_m}\\left[ {exp(-y\\alpha_mG_m(x))} \\right]  \\\\\n",
    "   & =E_{x\\sim D_m}\\left[ \\sum_{y_i=G_m(x_i)}{e^{-\\alpha_m}}+\\sum_{y_i\\ne G_m(x_i)}{e^{\\alpha_m}} \\right] \\\\\n",
    "   & =e^{-\\alpha_m}P(y_i=G_m(x_i))+e^{\\alpha_m}P(y_i\\ne G_m(x_i)) \\\\\n",
    "   & =e^{-\\alpha_m}(1-e_m)+e^{\\alpha_m}e_m \n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "首先，我们求解$G_m(x)$,对任意的$\\alpha>0$,最优的$G_m(x)$应为：\n",
    "\n",
    "$$G_{m}(x)=argmin\\sum_{i=1}^{N}{{w}_{mi}I(y_i\\ne G_m(x_i))}$$ \n",
    "\n",
    "其中$w_{mi}$是第$m$轮训练样本的权重。$G_m(x)$就是在第$m$轮中使得加权训练样本误差率最小的分类器。\n",
    "\n",
    "在得到$G_m(x)$后，我们来求$\\alpha_m$。$\\alpha_m$应该使得损失函数最小，所以令下式对$\\alpha_m$求导等于零：\n",
    "\n",
    "$$\\frac{\\alpha L(\\alpha_m,G_m(x))}{\\alpha\\alpha_m}=-e^{-\\alpha_{m}}(1-e_m)+e^{\\alpha_{m}}e_m=0$$ \n",
    "\n",
    "得到$\\alpha_m$的表达式：\n",
    "\n",
    "$$\\alpha_m=\\frac{1}{2}ln(\\frac{1-e_m}{e_m})$$ \n",
    "\n",
    "由于$e_m<0.5$，所以$\\alpha_m>0$,且$\\alpha_m$随着$e_m$的减小而增大。\n",
    "\n",
    "这时候我们就可以得到Adaboost的迭代公式：\n",
    "\n",
    "$$f_{m}(x)=f_{m-1}(x)+\\alpha_{m}G_{m}(x)$$ \n",
    "\n",
    "这时候就只剩下最后一个问题，之前说，我们会根据基学习器的表现对训练样本的权重进行调整，使得先前基学习器做错的样本在后续得到更多的关注。那训练样本的权重分布$D_m$应该怎么变化呢?\n",
    "\n",
    "这一部分在周志华老师《机器学习》$P175$有详细的推导，感兴趣的读者可以自行查阅。我这里直接给出$D_m$的迭代公式，并证明其可行性。\n",
    "\n",
    "$$\n",
    "\\left. \n",
    "\\begin{array}{ll}\n",
    "D_{m}=(w_{m,1},w_{m,2},...w_{m,N})\\\\\n",
    "D_{m+1}=(w_{m+1,1},w_{m+1,2},...w_{m+1,N})\\\\\n",
    "w_{m+1,i}=\\frac{w_{mi}exp(-\\alpha_my_iG_m(x_i))}{Z_m}\n",
    "\\end{array}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "其中$Z_m=\\sum_{i=1}^{N}{w_{mi}exp(-\\alpha_my_iG_m(x_i))}$，是一个常数。\n",
    "\n",
    "上面就是样本权重$D_m$的迭代公式。我们发现：\n",
    "\n",
    "当$G_m(x_i)= y_i$时：\n",
    "\n",
    "$$w_{m+1,i}=\\frac{w_{mi}}{Z_m}e^{-\\alpha_m}$$ \n",
    "\n",
    "当$G_m(x_i)\\ne y_i$时：\n",
    "\n",
    "$$w_{m+1,i}=\\frac{w_{mi}}{Z_m}e^{\\alpha_m}$$ \n",
    "\n",
    "由于$\\alpha_m>0$，从上面的式子可以看到，当样本$i$上一次被误分类时，其下一次的权重$w_{m+1,i}$会变大；而当样本$i$上一次被分类正确时，其下一次的权重 $w_{m+1,i}$会变小。因此，误分类样本在下一轮学习中起到的作用更大，这也是Adaboost的一个特点。\n",
    "\n",
    "以上就是Adaboost算法原理部分的全部内容，从基学习器到其系数，再到数据集合的权重迭代，我们都给出了较为详细的公式推导。希望给大家一些帮助。\n",
    "\n",
    "## Adaboost算法的误差\n",
    "先给大家结论：随着集成学习中个体分类器数目的增加，其集成的错误率将成指数级下降，最终趋向于零。\n",
    "\n",
    "当然，如果分类器数目过多也会发生过拟合现象，导致分类器的泛化能力不强，所以我们应该在其中寻求一种平衡。\n",
    "\n",
    "关于算法误差这个点，李航老师《统计学习方法》$P142$和周志华老师《机器学习》$P172$页都有解释。李航老师的证明更为详细。周志华老师的证明用到了$Hoeffding$不等式。\n",
    "\n",
    "最终得到的不等式：\n",
    "\n",
    "$$P(f(x)\\ne y)=\\sum_{k=0}^{N/2向下取整}{C_N^{k}(1-e)^k(e)^{N-k}}\\leq exp(-\\frac{1}{2}N(1-2e)^2)$$ \n",
    "\n",
    "我们发现，随着个体分类器数目的增加，其集成的错误率将成指数级下降。当然前提条件是每个基分类器的分类正确率要高于$50\\%$，即$e_i<0.5$。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
